#!/usr/bin/env python3
"""
ç®€å•å›å½’åˆ†æ - ä¸“é—¨é’ˆå¯¹å°æ ·æœ¬è¿‡æ‹Ÿåˆé—®é¢˜çš„è§£å†³æ–¹æ¡ˆ
ä½¿ç”¨ç®€å•æ¨¡å‹å’Œä¸¥æ ¼çš„äº¤å‰éªŒè¯
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import cross_val_score, LeaveOneOut
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

def analyze_relationship(x, y, feature_name, target_name):
    """åˆ†æä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œä½¿ç”¨å¤šç§æ–¹æ³•éªŒè¯"""
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(x) | np.isnan(y))
    x_clean = x[mask]
    y_clean = y[mask]
    
    if len(x_clean) < 5:
        return None
    
    print(f"\nğŸ“Š åˆ†æ: {feature_name} vs {target_name}")
    print(f"   æ ·æœ¬æ•°é‡: {len(x_clean)}")
    
    # 1. çš®å°”é€Šç›¸å…³ç³»æ•°
    pearson_r, pearson_p = stats.pearsonr(x_clean, y_clean)
    print(f"   çš®å°”é€Šç›¸å…³ç³»æ•°: r = {pearson_r:.4f}, p-value = {pearson_p:.4f}")
    
    # 2. ç®€å•çº¿æ€§å›å½’
    X = x_clean.reshape(-1, 1)
    
    # è®­ç»ƒæ¨¡å‹
    linear_model = LinearRegression()
    linear_model.fit(X, y_clean)
    y_pred_linear = linear_model.predict(X)
    
    # è®­ç»ƒRÂ²
    train_r2_linear = r2_score(y_clean, y_pred_linear)
    
    # ç•™ä¸€äº¤å‰éªŒè¯
    try:
        cv_scores_linear = cross_val_score(linear_model, X, y_clean, 
                                          cv=LeaveOneOut(), scoring='r2')
        # æ¸…ç†æ— æ•ˆåˆ†æ•°
        cv_scores_linear = cv_scores_linear[~np.isnan(cv_scores_linear) & ~np.isinf(cv_scores_linear)]
        cv_r2_linear = cv_scores_linear.mean() if len(cv_scores_linear) > 0 else 0.0
        cv_std_linear = cv_scores_linear.std() if len(cv_scores_linear) > 0 else 0.0
    except Exception as e:
        print(f"     äº¤å‰éªŒè¯å¤±è´¥: {e}")
        cv_scores_linear = np.array([0.0])
        cv_r2_linear = 0.0
        cv_std_linear = 0.0
    
    # è¿‡æ‹Ÿåˆç¨‹åº¦
    overfitting_linear = train_r2_linear - cv_r2_linear
    
    print(f"   çº¿æ€§å›å½’:")
    print(f"     è®­ç»ƒ RÂ²: {train_r2_linear:.4f}")
    print(f"     äº¤å‰éªŒè¯ RÂ²: {cv_r2_linear:.4f} Â± {cv_std_linear:.4f}")
    print(f"     è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting_linear:.4f}")
    
    # 3. Ridgeå›å½’ (æ­£åˆ™åŒ–)
    ridge_model = Ridge(alpha=1.0)
    ridge_model.fit(X, y_clean)
    y_pred_ridge = ridge_model.predict(X)
    
    train_r2_ridge = r2_score(y_clean, y_pred_ridge)
    try:
        cv_scores_ridge = cross_val_score(ridge_model, X, y_clean, 
                                         cv=LeaveOneOut(), scoring='r2')
        # æ¸…ç†æ— æ•ˆåˆ†æ•°
        cv_scores_ridge = cv_scores_ridge[~np.isnan(cv_scores_ridge) & ~np.isinf(cv_scores_ridge)]
        cv_r2_ridge = cv_scores_ridge.mean() if len(cv_scores_ridge) > 0 else 0.0
    except Exception as e:
        print(f"     Ridgeäº¤å‰éªŒè¯å¤±è´¥: {e}")
        cv_scores_ridge = np.array([0.0])
        cv_r2_ridge = 0.0
    overfitting_ridge = train_r2_ridge - cv_r2_ridge
    
    print(f"   Ridgeå›å½’ (Î±=1.0):")
    print(f"     è®­ç»ƒ RÂ²: {train_r2_ridge:.4f}")
    print(f"     äº¤å‰éªŒè¯ RÂ²: {cv_r2_ridge:.4f}")
    print(f"     è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting_ridge:.4f}")
    
    # 4. æ£€éªŒæ•°æ®èŒƒå›´å’Œåˆ†å¸ƒ
    x_range = x_clean.max() - x_clean.min()
    x_cv = np.std(x_clean) / np.mean(x_clean)
    y_range = y_clean.max() - y_clean.min()
    y_cv = np.std(y_clean) / np.mean(y_clean)
    
    print(f"   æ•°æ®ç‰¹å¾:")
    print(f"     Xå˜å¼‚ç³»æ•°: {x_cv:.4f}")
    print(f"     Yå˜å¼‚ç³»æ•°: {y_cv:.4f}")
    print(f"     XèŒƒå›´/å‡å€¼: {x_range/np.mean(x_clean):.4f}")
    print(f"     YèŒƒå›´/å‡å€¼: {y_range/np.mean(y_clean):.4f}")
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    if cv_r2_ridge > cv_r2_linear and overfitting_ridge < overfitting_linear:
        best_model = "Ridge"
        best_r2 = cv_r2_ridge
        best_overfitting = overfitting_ridge
        best_model_obj = ridge_model
    else:
        best_model = "Linear"
        best_r2 = cv_r2_linear
        best_overfitting = overfitting_linear
        best_model_obj = linear_model
    
    # è´¨é‡è¯„ä¼°
    if abs(pearson_r) < 0.2:
        quality = "å¼±ç›¸å…³"
    elif best_overfitting > 0.3:
        quality = "è¿‡æ‹Ÿåˆä¸¥é‡"
    elif best_r2 < 0.1:
        quality = "æ¨¡å‹æ•ˆæœå·®"
    elif best_r2 > 0.3 and best_overfitting < 0.1:
        quality = "è‰¯å¥½"
    else:
        quality = "ä¸€èˆ¬"
    
    print(f"   æœ€ä½³æ¨¡å‹: {best_model}")
    print(f"   æ¨¡å‹è´¨é‡: {quality}")
    
    return {
        'feature': feature_name,
        'target': target_name,
        'sample_size': len(x_clean),
        'pearson_r': pearson_r,
        'pearson_p': pearson_p,
        'linear_train_r2': train_r2_linear,
        'linear_cv_r2': cv_r2_linear,
        'linear_overfitting': overfitting_linear,
        'ridge_train_r2': train_r2_ridge,
        'ridge_cv_r2': cv_r2_ridge,
        'ridge_overfitting': overfitting_ridge,
        'best_model': best_model,
        'best_cv_r2': best_r2,
        'best_overfitting': best_overfitting,
        'quality': quality,
        'x_cv': x_cv,
        'y_cv': y_cv
    }

def create_diagnostic_plot(df, x_col, y_col, output_dir):
    """åˆ›å»ºè¯Šæ–­å›¾è¡¨"""
    
    x = df[x_col].values
    y = df[y_col].values
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(x) | np.isnan(y))
    x_clean = x[mask]
    y_clean = y[mask]
    
    if len(x_clean) < 5:
        return None
    
    # åˆ›å»ºå›¾å½¢
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. åŸå§‹æ•£ç‚¹å›¾ + çº¿æ€§å›å½’
    ax1.scatter(x_clean, y_clean, alpha=0.7, s=60, color='steelblue', edgecolors='black')
    
    # æ‹Ÿåˆçº¿æ€§å›å½’
    X = x_clean.reshape(-1, 1)
    model = LinearRegression()
    model.fit(X, y_clean)
    x_line = np.linspace(x_clean.min(), x_clean.max(), 100)
    y_line = model.predict(x_line.reshape(-1, 1))
    ax1.plot(x_line, y_line, 'r-', linewidth=2, label='çº¿æ€§å›å½’')
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    train_r2 = r2_score(y_clean, model.predict(X))
    try:
        cv_scores = cross_val_score(model, X, y_clean, cv=LeaveOneOut(), scoring='r2')
        # æ¸…ç†æ— æ•ˆåˆ†æ•°
        cv_scores = cv_scores[~np.isnan(cv_scores) & ~np.isinf(cv_scores)]
        cv_r2 = cv_scores.mean() if len(cv_scores) > 0 else 0.0
    except Exception as e:
        print(f"     è¯Šæ–­å›¾äº¤å‰éªŒè¯å¤±è´¥: {e}")
        cv_scores = np.array([0.0])
        cv_r2 = 0.0
    pearson_r, _ = stats.pearsonr(x_clean, y_clean)
    
    ax1.set_title(f'{x_col} vs {y_col}')
    ax1.set_xlabel(x_col)
    ax1.set_ylabel(y_col)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    stats_text = f'è®­ç»ƒRÂ²: {train_r2:.3f}\näº¤å‰éªŒè¯RÂ²: {cv_r2:.3f}\nçš®å°”é€Šr: {pearson_r:.3f}'
    ax1.text(0.05, 0.95, stats_text, transform=ax1.transAxes, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    # 2. æ®‹å·®å›¾
    residuals = y_clean - model.predict(X)
    ax2.scatter(model.predict(X), residuals, alpha=0.7, s=60, color='green')
    ax2.axhline(y=0, color='red', linestyle='--')
    ax2.set_title('æ®‹å·®å›¾')
    ax2.set_xlabel('é¢„æµ‹å€¼')
    ax2.set_ylabel('æ®‹å·®')
    ax2.grid(True, alpha=0.3)
    
    # 3. Q-Qå›¾æ£€éªŒæ®‹å·®æ­£æ€æ€§
    stats.probplot(residuals, dist="norm", plot=ax3)
    ax3.set_title('æ®‹å·®Q-Qå›¾')
    ax3.grid(True, alpha=0.3)
    
    # 4. äº¤å‰éªŒè¯åˆ†æ•°åˆ†å¸ƒ
    # æ£€æŸ¥å¹¶æ¸…ç†cv_scoresæ•°æ®
    cv_scores_clean = cv_scores[~np.isnan(cv_scores) & ~np.isinf(cv_scores)]
    
    if len(cv_scores_clean) > 0 and np.std(cv_scores_clean) > 1e-10:
        # åŠ¨æ€ç¡®å®šbinsæ•°é‡
        n_bins = min(10, max(3, len(cv_scores_clean) // 2))
        ax4.hist(cv_scores_clean, bins=n_bins, alpha=0.7, color='orange', edgecolor='black')
        ax4.axvline(cv_r2, color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {cv_r2:.3f}')
        ax4.set_title('äº¤å‰éªŒè¯RÂ²åˆ†å¸ƒ')
        ax4.set_xlabel('RÂ²')
        ax4.set_ylabel('é¢‘æ¬¡')
        ax4.legend()
    else:
        # å¦‚æœæ•°æ®æ— æ•ˆï¼Œæ˜¾ç¤ºæ–‡æœ¬è¯´æ˜
        ax4.text(0.5, 0.5, f'äº¤å‰éªŒè¯æ•°æ®:\nå¹³å‡RÂ²: {cv_r2:.3f}\næ ·æœ¬æ•°: {len(cv_scores)}', 
                ha='center', va='center', transform=ax4.transAxes,
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        ax4.set_title('äº¤å‰éªŒè¯RÂ²ä¿¡æ¯')
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
    
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    filename = f"{x_col}_{y_col}_diagnostic.png"
    filepath = os.path.join(output_dir, filename)
    plt.savefig(filepath, dpi=200, bbox_inches='tight')
    plt.close()
    
    return filepath

def main():
    print("ğŸ” å¼€å§‹ç®€å•å›å½’åˆ†æ - é˜²è¿‡æ‹Ÿåˆç‰ˆæœ¬")
    
    # è®¾ç½®è·¯å¾„
    data_file = "/home/ubuntu/project/MSC/Msc_Project/models/input_1-100/merged_dataset.csv"
    output_dir = "/home/ubuntu/project/MSC/Msc_Project/models/analysis_simple"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {data_file}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åŠ è½½æ•°æ®
    try:
        df = pd.read_csv(data_file)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # å®šä¹‰ç‰¹å¾å’Œç›®æ ‡å˜é‡
    layout_features = [
        'avg_dist_to_center', 'std_nearest_neighbor', 'min_distance',
        'max_pairwise_distance', 'cs_density_std', 'cluster_count',
        'coverage_ratio', 'max_gap_distance', 'gini_coefficient',
        'avg_betweenness_centrality'
    ]
    
    performance_metrics = [
        'duration_mean', 'waiting_time_mean', 'charging_time_mean',
        'energy_gini', 'vehicle_gini', 'charging_station_coverage',
        'reroute_count', 'ev_charging_participation_rate'
    ]
    
    print(f"ğŸ“Š å¸ƒå±€ç‰¹å¾: {len(layout_features)} ä¸ª")
    print(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡: {len(performance_metrics)} ä¸ª")
    
    # åˆ†ææ‰€æœ‰ç»„åˆ
    results = []
    
    print(f"\nğŸ” å¼€å§‹åˆ†æ {len(layout_features) * len(performance_metrics)} ä¸ªç»„åˆ...")
    
    for i, feature in enumerate(layout_features):
        print(f"\n[{i+1}/{len(layout_features)}] å¤„ç†ç‰¹å¾: {feature}")
        
        for j, target in enumerate(performance_metrics):
            if feature in df.columns and target in df.columns:
                # åˆ†æå…³ç³»
                result = analyze_relationship(
                    df[feature].values, 
                    df[target].values, 
                    feature, 
                    target
                )
                
                if result:
                    results.append(result)
                    
                    # ä¸ºè´¨é‡å¥½çš„å…³ç³»åˆ›å»ºè¯Šæ–­å›¾
                    if result['quality'] in ['è‰¯å¥½', 'ä¸€èˆ¬'] and abs(result['pearson_r']) > 0.15:
                        print(f"     åˆ›å»ºè¯Šæ–­å›¾...")
                        create_diagnostic_plot(df, feature, target, output_dir)
    
    # ä¿å­˜ç»“æœ
    if results:
        results_df = pd.DataFrame(results)
        results_file = os.path.join(output_dir, "regression_analysis_results.csv")
        results_df.to_csv(results_file, index=False)
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±åˆ†æ: {len(results)} ä¸ªå…³ç³»")
        print(f"ğŸ’¾ ç»“æœä¿å­˜åˆ°: {results_file}")
        
        # æ±‡æ€»ç»Ÿè®¡
        print(f"\nğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
        quality_counts = results_df['quality'].value_counts()
        for quality, count in quality_counts.items():
            print(f"   {quality}: {count} ä¸ª")
        
        print(f"\nğŸ† æœ€ä½³å…³ç³» (æŒ‰äº¤å‰éªŒè¯RÂ²æ’åº):")
        best_results = results_df.sort_values('best_cv_r2', ascending=False).head(10)
        for _, row in best_results.iterrows():
            print(f"   {row['feature']} -> {row['target']}: "
                  f"CV RÂ² = {row['best_cv_r2']:.4f}, "
                  f"è¿‡æ‹Ÿåˆ = {row['best_overfitting']:.4f}, "
                  f"è´¨é‡ = {row['quality']}")
        
        print(f"\nâš ï¸ è¿‡æ‹Ÿåˆä¸¥é‡çš„å…³ç³»:")
        overfitting_issues = results_df[results_df['best_overfitting'] > 0.2]
        for _, row in overfitting_issues.iterrows():
            print(f"   {row['feature']} -> {row['target']}: "
                  f"è¿‡æ‹Ÿåˆ = {row['best_overfitting']:.4f}")
        
        print(f"\nğŸ“Š å»ºè®®:")
        good_quality = len(results_df[results_df['quality'] == 'è‰¯å¥½'])
        if good_quality > 0:
            print(f"   âœ… å‘ç° {good_quality} ä¸ªé«˜è´¨é‡å…³ç³»ï¼Œå¯ç”¨äºè®ºæ–‡")
        else:
            print(f"   âš ï¸ æ²¡æœ‰å‘ç°é«˜è´¨é‡å…³ç³»ï¼Œæ•°æ®å¯èƒ½å­˜åœ¨ä»¥ä¸‹é—®é¢˜:")
            print(f"      - æ ·æœ¬é‡å¤ªå° (N={len(df)})")
            print(f"      - å˜é‡ä¹‹é—´çœŸå®ç›¸å…³æ€§è¾ƒå¼±")
            print(f"      - éœ€è¦æ›´å¤šæ•°æ®æˆ–ç‰¹å¾å·¥ç¨‹")
    
    return 0

if __name__ == '__main__':
    exit(main())
