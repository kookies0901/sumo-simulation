#!/usr/bin/env python3
"""
åˆå¹¶å……ç”µæ¡©å¸ƒå±€ç‰¹å¾å’Œæ€§èƒ½æŒ‡æ ‡æ•°æ®
å°†all_layout_features.csvå’Œbatch_charging_analysis.csvæŒ‰ç…§layout_idåˆå¹¶
"""

import os
import pandas as pd
import argparse

def load_layout_features(features_file):
    """åŠ è½½å¸ƒå±€ç‰¹å¾æ•°æ®"""
    try:
        df = pd.read_csv(features_file)
        print(f"âœ… å¸ƒå±€ç‰¹å¾æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
        print(f"ğŸ“Š ç‰¹å¾åˆ—æ•°: {len(df.columns)}")
        print(f"ğŸ·ï¸ å¸ƒå±€IDèŒƒå›´: {df['layout_id'].min()} - {df['layout_id'].max()}")
        return df
    except Exception as e:
        print(f"âŒ åŠ è½½å¸ƒå±€ç‰¹å¾å¤±è´¥: {e}")
        return None

def load_performance_metrics(metrics_file):
    """åŠ è½½æ€§èƒ½æŒ‡æ ‡æ•°æ®"""
    try:
        df = pd.read_csv(metrics_file)
        print(f"âœ… æ€§èƒ½æŒ‡æ ‡æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
        print(f"ğŸ“Š æŒ‡æ ‡åˆ—æ•°: {len(df.columns)}")
        
        # æ£€æŸ¥layout_idåˆ—å
        layout_id_cols = [col for col in df.columns if 'layout_id' in col.lower()]
        print(f"ğŸ·ï¸ å‘ç°layout_idç›¸å…³åˆ—: {layout_id_cols}")
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªåŒ…å«layout_idçš„åˆ—ä½œä¸ºåˆå¹¶é”®
        if layout_id_cols:
            layout_id_col = layout_id_cols[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ª
            if layout_id_col != 'layout_id':
                df = df.rename(columns={layout_id_col: 'layout_id'})
                print(f"ğŸ”„ é‡å‘½ååˆ—: {layout_id_col} -> layout_id")
        
        if 'layout_id' in df.columns:
            print(f"ğŸ·ï¸ å¸ƒå±€IDèŒƒå›´: {df['layout_id'].min()} - {df['layout_id'].max()}")
        else:
            print("âŒ æœªæ‰¾åˆ°layout_idåˆ—")
            
        return df
    except Exception as e:
        print(f"âŒ åŠ è½½æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
        return None

def merge_datasets(features_df, metrics_df):
    """åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†"""
    try:
        print("\nğŸ”„ å¼€å§‹åˆå¹¶æ•°æ®é›†...")
        
        # æ£€æŸ¥åˆå¹¶å‰çš„æ•°æ®
        print(f"ğŸ“Š ç‰¹å¾æ•°æ®: {len(features_df)} è¡Œ, {len(features_df.columns)} åˆ—")
        print(f"ğŸ“Š æŒ‡æ ‡æ•°æ®: {len(metrics_df)} è¡Œ, {len(metrics_df.columns)} åˆ—")
        
        # æ£€æŸ¥layout_idçš„äº¤é›†
        features_ids = set(features_df['layout_id'])
        metrics_ids = set(metrics_df['layout_id'])
        
        common_ids = features_ids.intersection(metrics_ids)
        print(f"ğŸ¯ å…±åŒçš„layout_idæ•°é‡: {len(common_ids)}")
        print(f"ğŸ” ç‰¹å¾æ•°æ®ç‹¬æœ‰: {len(features_ids - metrics_ids)}")
        print(f"ğŸ” æŒ‡æ ‡æ•°æ®ç‹¬æœ‰: {len(metrics_ids - features_ids)}")
        
        if len(common_ids) == 0:
            print("âŒ æ²¡æœ‰å…±åŒçš„layout_idï¼Œæ— æ³•åˆå¹¶")
            return None
        
        # æ‰§è¡Œå†…è¿æ¥åˆå¹¶
        merged_df = pd.merge(features_df, metrics_df, on='layout_id', how='inner')
        
        print(f"âœ… åˆå¹¶å®Œæˆ: {len(merged_df)} è¡Œ, {len(merged_df.columns)} åˆ—")
        print(f"ğŸ‰ æˆåŠŸåˆå¹¶ç‡: {len(merged_df)/len(common_ids)*100:.1f}%")
        
        return merged_df
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return None

def save_merged_dataset(merged_df, output_file):
    """ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†"""
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        merged_df.to_csv(output_file, index=False)
        print(f"ğŸ’¾ åˆå¹¶æ•°æ®é›†å·²ä¿å­˜: {output_file}")
        
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        print(f"\nğŸ“‹ æœ€ç»ˆæ•°æ®é›†ä¿¡æ¯:")
        print(f"   - æ€»è¡Œæ•°: {len(merged_df)}")
        print(f"   - æ€»åˆ—æ•°: {len(merged_df.columns)}")
        print(f"   - æ–‡ä»¶å¤§å°: {os.path.getsize(output_file)/1024:.1f} KB")
        
        # æ˜¾ç¤ºåˆ—å
        print(f"\nğŸ“Š æ•°æ®åˆ—æ¦‚è§ˆ:")
        feature_cols = []
        metric_cols = []
        
        for col in merged_df.columns:
            if col == 'layout_id':
                continue
            elif col in ['cs_count', 'avg_dist_to_center', 'avg_nearest_neighbor', 
                        'std_nearest_neighbor', 'min_distance', 'max_pairwise_distance',
                        'cs_density_std', 'cluster_count', 'coverage_ratio', 
                        'max_gap_distance', 'gini_coefficient', 'avg_betweenness_centrality']:
                feature_cols.append(col)
            else:
                metric_cols.append(col)
        
        print(f"   ğŸ—ï¸ å¸ƒå±€ç‰¹å¾ ({len(feature_cols)}): {', '.join(feature_cols[:5])}{'...' if len(feature_cols) > 5 else ''}")
        print(f"   ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ ({len(metric_cols)}): {', '.join(metric_cols[:5])}{'...' if len(metric_cols) > 5 else ''}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='åˆå¹¶å……ç”µæ¡©å¸ƒå±€ç‰¹å¾å’Œæ€§èƒ½æŒ‡æ ‡æ•°æ®')
    parser.add_argument('--input_dir', type=str, 
                       default='/home/ubuntu/project/MSC/Msc_Project/models/input',
                       help='è¾“å…¥æ–‡ä»¶ç›®å½•')
    parser.add_argument('--features_file', type=str,
                       default='all_layout_features.csv',
                       help='å¸ƒå±€ç‰¹å¾æ–‡ä»¶å')
    parser.add_argument('--metrics_file', type=str,
                       default='batch_charging_analysis.csv',
                       help='æ€§èƒ½æŒ‡æ ‡æ–‡ä»¶å')
    parser.add_argument('--output_file', type=str,
                       default='merged_dataset.csv',
                       help='è¾“å‡ºæ–‡ä»¶å')
    
    args = parser.parse_args()
    
    # æ„å»ºå®Œæ•´è·¯å¾„
    features_path = os.path.join(args.input_dir, args.features_file)
    metrics_path = os.path.join(args.input_dir, args.metrics_file)
    output_path = os.path.join(args.input_dir, args.output_file)
    
    print("ğŸš€ å¼€å§‹åˆå¹¶å……ç”µæ¡©å¸ƒå±€ç‰¹å¾å’Œæ€§èƒ½æŒ‡æ ‡æ•°æ®")
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"ğŸ—ï¸ ç‰¹å¾æ–‡ä»¶: {features_path}")
    print(f"ğŸ“ˆ æŒ‡æ ‡æ–‡ä»¶: {metrics_path}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_path}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(features_path):
        print(f"âŒ ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {features_path}")
        return 1
    
    if not os.path.exists(metrics_path):
        print(f"âŒ æŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {metrics_path}")
        return 1
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“– åŠ è½½å¸ƒå±€ç‰¹å¾æ•°æ®...")
    features_df = load_layout_features(features_path)
    if features_df is None:
        return 1
    
    print(f"\nğŸ“– åŠ è½½æ€§èƒ½æŒ‡æ ‡æ•°æ®...")
    metrics_df = load_performance_metrics(metrics_path)
    if metrics_df is None:
        return 1
    
    # åˆå¹¶æ•°æ®
    merged_df = merge_datasets(features_df, metrics_df)
    if merged_df is None:
        return 1
    
    # ä¿å­˜ç»“æœ
    success = save_merged_dataset(merged_df, output_path)
    if not success:
        return 1
    
    print(f"\nğŸ‰ æ•°æ®åˆå¹¶å®Œæˆï¼")
    print(f"ğŸ“Š æœ€ç»ˆæ•°æ®é›†åŒ…å« {len(merged_df)} ä¸ªå……ç”µæ¡©å¸ƒå±€çš„å®Œæ•´ç‰¹å¾å’Œæ€§èƒ½æ•°æ®")
    
    return 0

if __name__ == '__main__':
    exit(main())
