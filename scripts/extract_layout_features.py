import os
import json
import argparse
import xml.etree.ElementTree as ET
import pandas as pd
import numpy as np
from collections import defaultdict
from math import sqrt
from scipy.spatial.distance import pdist, squareform
import networkx as nx
from sklearn.cluster import DBSCAN

def extract_layout_features_from_json(cs_group_data, net_xml_path):
    """ä»JSONæ•°æ®ä¸­æå–å¸ƒå±€ç‰¹å¾"""
    # Load edge information from net.xml
    edge_shapes = {}
    net_tree = ET.parse(net_xml_path)
    
    # æå–è¾¹çš„å‡ ä½•ä¿¡æ¯
    for edge in net_tree.findall(".//edge"):
        edge_id = edge.get("id")
        for lane in edge.findall("lane"):
            lane_id = lane.get("id")
            shape = lane.get("shape")
            if shape and edge_id:
                # è§£æshapeå­—ç¬¦ä¸²ï¼Œè·å–åæ ‡ç‚¹åˆ—è¡¨
                points = []
                for point_str in shape.split():
                    x, y = map(float, point_str.split(','))
                    points.append((x, y))
                edge_shapes[edge_id] = points
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªlaneä»£è¡¨è¿™æ¡edge
                break
    
    # è®¡ç®—å……ç”µæ¡©çš„å®é™…åæ ‡
    cs_coords = []
    for cs_data in cs_group_data:
        edge_id = cs_data["edge_id"]
        pos = cs_data["pos"]
        
        if edge_id in edge_shapes:
            # æ ¹æ®positionåœ¨edgeä¸Šæ’å€¼å¾—åˆ°åæ ‡
            coord = interpolate_position_on_edge(edge_shapes[edge_id], pos)
            if coord:
                cs_coords.append(coord)

    # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•åæ ‡å°±è¿”å›ç©º
    if not cs_coords:
        raise ValueError(f"No valid charging station coordinates found")
    
    # è°ƒç”¨åŸæœ‰çš„ç‰¹å¾è®¡ç®—é€»è¾‘
    return calculate_features_from_coords(cs_coords, net_xml_path)

def interpolate_position_on_edge(edge_points, position):
    """åœ¨è¾¹ä¸Šæ ¹æ®ä½ç½®æ’å€¼å¾—åˆ°åæ ‡"""
    try:
        if len(edge_points) < 2:
            return edge_points[0] if edge_points else None
        
        # è®¡ç®—è¾¹çš„æ€»é•¿åº¦
        total_length = 0
        segment_lengths = []
        for i in range(len(edge_points) - 1):
            p1 = np.array(edge_points[i])
            p2 = np.array(edge_points[i + 1])
            length = np.linalg.norm(p2 - p1)
            segment_lengths.append(length)
            total_length += length
        
        if total_length == 0:
            return edge_points[0]
        
        # å¦‚æœpositionè¶…å‡ºè¾¹é•¿ï¼Œè¿”å›ç«¯ç‚¹
        if position <= 0:
            return edge_points[0]
        if position >= total_length:
            return edge_points[-1]
        
        # æ‰¾åˆ°positionå¯¹åº”çš„çº¿æ®µ
        current_length = 0
        for i, seg_length in enumerate(segment_lengths):
            if current_length + seg_length >= position:
                # åœ¨å½“å‰çº¿æ®µä¸Šæ’å€¼
                t = (position - current_length) / seg_length if seg_length > 0 else 0
                p1 = np.array(edge_points[i])
                p2 = np.array(edge_points[i + 1])
                interpolated = p1 + t * (p2 - p1)
                return tuple(interpolated)
            current_length += seg_length
        
        # é»˜è®¤è¿”å›æœ€åä¸€ä¸ªç‚¹
        return edge_points[-1]
        
    except Exception as e:
        print(f"Warning: æ’å€¼è®¡ç®—å¤±è´¥: {e}")
        return edge_points[0] if edge_points else None

def calculate_features_from_coords(cs_coords, net_xml_path):
    """ä»å……ç”µæ¡©åæ ‡è®¡ç®—ç‰¹å¾"""
    # è½¬æˆ numpy array å¤„ç†
    coords = np.array(cs_coords)
    cs_count = len(coords)

    # ä¸­å¿ƒç‚¹
    center_x, center_y = coords.mean(axis=0)

    # è®¡ç®—ç‰¹å¾
    dists_to_center = np.linalg.norm(coords - [center_x, center_y], axis=1)
    avg_dist_to_center = dists_to_center.mean()

    if cs_count > 1:
        pairwise_dists = pdist(coords)
        avg_nn = pairwise_dists.mean()
        std_nn = pairwise_dists.std()
        min_nn = pairwise_dists.min()
        
        # æ–°å¢ç‰¹å¾ï¼šæœ€å¤§ä¸¤ä¸¤è·ç¦»
        max_pairwise_distance = pairwise_dists.max()
        
        # æ–°å¢ç‰¹å¾ï¼šè®¡ç®—å¯†åº¦æ ‡å‡†å·®
        # å°†åŒºåŸŸåˆ’åˆ†ä¸ºç½‘æ ¼ï¼Œè®¡ç®—æ¯ä¸ªç½‘æ ¼çš„æ¡©æ•°å¯†åº¦
        x_coords = coords[:, 0]
        y_coords = coords[:, 1]
        
        # è®¡ç®—è¾¹ç•Œ
        x_min, x_max = x_coords.min(), x_coords.max()
        y_min, y_max = y_coords.min(), y_coords.max()
        
        # åˆ’åˆ†ç½‘æ ¼ï¼ˆä½¿ç”¨10x10ç½‘æ ¼ï¼‰
        grid_size = 10
        x_bins = np.linspace(x_min, x_max, grid_size + 1)
        y_bins = np.linspace(y_min, y_max, grid_size + 1)
        
        # è®¡ç®—æ¯ä¸ªç½‘æ ¼çš„æ¡©æ•°
        grid_counts = np.zeros((grid_size, grid_size))
        for i in range(cs_count):
            x_idx = np.digitize(x_coords[i], x_bins) - 1
            y_idx = np.digitize(y_coords[i], y_bins) - 1
            if 0 <= x_idx < grid_size and 0 <= y_idx < grid_size:
                grid_counts[x_idx, y_idx] += 1
        
        # è®¡ç®—æ¯ä¸ªç½‘æ ¼çš„é¢ç§¯ï¼ˆå¹³æ–¹å…¬é‡Œï¼‰
        grid_area_km2 = ((x_max - x_min) / grid_size) * ((y_max - y_min) / grid_size) / 1000000
        
        # è®¡ç®—å¯†åº¦ï¼ˆæ¡©æ•°/å¹³æ–¹å…¬é‡Œï¼‰
        densities = grid_counts.flatten() / grid_area_km2
        # è¿‡æ»¤æ‰æ²¡æœ‰æ¡©çš„ç½‘æ ¼
        densities = densities[densities > 0]
        
        if len(densities) > 0:
            cs_density_std = np.std(densities)
        else:
            cs_density_std = 0.0
            
        # æ–°å¢ç‰¹å¾ï¼šèšç±»ç°‡æ•°ï¼ˆä½¿ç”¨DBSCANç®—æ³•ï¼‰
        cluster_count = calculate_dbscan_cluster_count(coords)
            
    else:
        avg_nn = std_nn = min_nn = 0.0
        max_pairwise_distance = 0.0
        cs_density_std = 0.0
        cluster_count = 0 if cs_count == 0 else 1

    # æ–°å¢ç‰¹å¾ï¼šç©ºé—´è¦†ç›–ä¸å‡è¡¡åº¦æŒ‡æ ‡
    coverage_ratio, max_gap_distance, gini_coefficient = calculate_enhanced_coverage_features(
        cs_coords, net_xml_path
    )
    
    # æ–°å¢ç‰¹å¾ï¼šç½‘ç»œä½ç½®æŒ‡æ ‡ï¼ˆä½¿ç”¨NetworkXï¼‰
    avg_betweenness_centrality = calculate_network_centrality(cs_coords, {}, net_xml_path)

    # æ„å»ºè¾“å‡ºå­—å…¸
    features = {
        "cs_count": cs_count,
        "avg_dist_to_center": avg_dist_to_center,
        "avg_nearest_neighbor": avg_nn,
        "std_nearest_neighbor": std_nn,
        "min_distance": min_nn,
        "max_pairwise_distance": max_pairwise_distance,
        "cs_density_std": cs_density_std,
        # æ–°å¢ç‰¹å¾
        "cluster_count": cluster_count,
        "coverage_ratio": coverage_ratio,
        "max_gap_distance": max_gap_distance,
        "gini_coefficient": gini_coefficient,
        "avg_betweenness_centrality": avg_betweenness_centrality
    }

    return features

def extract_layout_features(cs_xml_path, net_xml_path):
    """ä»XMLæ–‡ä»¶ä¸­æå–å¸ƒå±€ç‰¹å¾ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    # Load all lane positions from net.xml
    lane_coords = {}
    net_tree = ET.parse(net_xml_path)
    for lane in net_tree.findall(".//lane"):
        lane_id = lane.get("id")
        shape = lane.get("shape")
        if shape:
            # å–shapeç¬¬ä¸€ä¸ªç‚¹ä½œä¸ºä»£è¡¨åæ ‡
            x, y = map(float, shape.split()[0].split(','))
            lane_coords[lane_id] = (x, y)

    # Load charging stations from cs xml
    cs_coords = []
    cs_tree = ET.parse(cs_xml_path)
    for cs in cs_tree.findall(".//chargingStation"):
        lane_id = cs.get("lane")
        if lane_id in lane_coords:
            cs_coords.append(lane_coords[lane_id])

    # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•åæ ‡å°±è¿”å›ç©º
    if not cs_coords:
        raise ValueError(f"No valid chargingStation coordinates found in {cs_xml_path}")

    return calculate_features_from_coords(cs_coords, net_xml_path)

def calculate_dbscan_cluster_count(coords):
    """ä½¿ç”¨DBSCANç®—æ³•è¿›è¡Œèšç±»åˆ†æ"""
    try:
        if len(coords) <= 1:
            return 0 if len(coords) == 0 else 1
        
        # ä½¿ç”¨DBSCANèšç±»ç®—æ³•
        # eps: é‚»åŸŸåŠå¾„ï¼ˆ500ç±³ï¼‰ï¼Œmin_samples: æœ€å°æ ·æœ¬æ•°
        eps = 500  # 500ç±³é‚»åŸŸåŠå¾„
        min_samples = 2  # æœ€å°‘2ä¸ªç‚¹å½¢æˆä¸€ä¸ªç°‡
        
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        cluster_labels = dbscan.fit_predict(coords)
        
        # è®¡ç®—èšç±»æ•°é‡ï¼ˆæ’é™¤å™ªå£°ç‚¹ï¼Œæ ‡ç­¾ä¸º-1ï¼‰
        unique_labels = set(cluster_labels)
        cluster_count = len(unique_labels) - (1 if -1 in unique_labels else 0)
        
        # å¦‚æœæ²¡æœ‰å½¢æˆèšç±»ï¼Œåˆ™æ¯ä¸ªç‚¹éƒ½æ˜¯ç‹¬ç«‹çš„èšç±»
        if cluster_count == 0:
            cluster_count = len(coords)
        
        return cluster_count
        
    except Exception as e:
        print(f"Warning: DBSCANèšç±»è®¡ç®—å¤±è´¥: {e}")
        # å›é€€åˆ°ç®€å•æ–¹æ³•
        return calculate_simple_cluster_count(coords)

def calculate_simple_cluster_count(coords):
    """ç®€åŒ–ç‰ˆæœ¬çš„èšç±»è®¡æ•°ï¼ŒåŸºäºè·ç¦»é˜ˆå€¼ï¼ˆä½œä¸ºDBSCANçš„å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    try:
        if len(coords) <= 1:
            return len(coords)
        
        # ä½¿ç”¨ç®€å•çš„è·ç¦»é˜ˆå€¼æ–¹æ³•
        threshold = 500  # 500ç±³é˜ˆå€¼ï¼Œä¸DBSCANä¿æŒä¸€è‡´
        clusters = []
        visited = set()
        
        for i in range(len(coords)):
            if i in visited:
                continue
                
            # å¼€å§‹æ–°èšç±»
            cluster = [i]
            visited.add(i)
            
            # æŸ¥æ‰¾æ‰€æœ‰åœ¨é˜ˆå€¼å†…çš„ç‚¹
            for j in range(i + 1, len(coords)):
                if j not in visited:
                    dist = np.linalg.norm(coords[i] - coords[j])
                    if dist <= threshold:
                        cluster.append(j)
                        visited.add(j)
            
            clusters.append(cluster)
        
        return len(clusters)
        
    except:
        return 0

# å…¨å±€ç¼“å­˜é‡‡æ ·ç‚¹
_cached_sample_coords = None

def calculate_enhanced_coverage_features(cs_coords, net_xml_path):
    """å¢å¼ºç‰ˆçš„è¦†ç›–ç‰¹å¾è®¡ç®—ï¼Œç›´æ¥ä½¿ç”¨ç½‘ç»œæ–‡ä»¶ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
    global _cached_sample_coords
    
    try:
        # ä½¿ç”¨ç¼“å­˜çš„é‡‡æ ·ç‚¹
        if _cached_sample_coords is None:
            print("ğŸ”§ é¦–æ¬¡é‡‡æ ·é“è·¯ç‚¹...")
            _cached_sample_coords = sample_road_points_from_network(net_xml_path)
            print("âœ… é“è·¯ç‚¹é‡‡æ ·å®Œæˆï¼Œåç»­åœºæ™¯å°†ä½¿ç”¨ç¼“å­˜")
        
        edge_sample_coords = _cached_sample_coords
        
        if not edge_sample_coords:
            return 0.0, 0.0, 0.0
        
        # è®¡ç®—æ¯ä¸ªé‡‡æ ·ç‚¹åˆ°æœ€è¿‘å……ç”µæ¡©çš„è·ç¦»
        distances_to_cs = []
        for edge_coord in edge_sample_coords:
            # è®¡ç®—åˆ°æ‰€æœ‰å……ç”µæ¡©çš„è·ç¦»ï¼Œå–æœ€å°å€¼
            dists = [np.linalg.norm(np.array(edge_coord) - np.array(cs_coord)) 
                    for cs_coord in cs_coords]
            min_dist = min(dists)
            distances_to_cs.append(min_dist)
        
        distances_to_cs = np.array(distances_to_cs)
        
        # 1. Coverage ratio (500må†…å¯åˆ°è¾¾å……ç”µæ¡©çš„è·¯æ®µæ¯”ä¾‹)
        coverage_ratio = np.mean(distances_to_cs <= 500)
        
        # 2. Max gap distance (æœ€å¤§æœåŠ¡ç©ºç™½è·ç¦»)
        max_gap_distance = np.max(distances_to_cs)
        
        # 3. Gini coefficient for service accessibility
        gini_coefficient = calculate_gini_coefficient(distances_to_cs)
        
        return coverage_ratio, max_gap_distance, gini_coefficient
        
    except Exception as e:
        print(f"Warning: è®¡ç®—å¢å¼ºè¦†ç›–ç‰¹å¾æ—¶å‡ºé”™: {e}")
        return 0.0, 0.0, 0.0

def sample_road_points_from_network(net_xml_path, sample_ratio=0.1):
    """ä»ç½‘ç»œæ–‡ä»¶ä¸­é‡‡æ ·é“è·¯ç‚¹"""
    try:
        tree = ET.parse(net_xml_path)
        root = tree.getroot()
        
        sample_coords = []
        
        # éå†æ‰€æœ‰è¾¹ï¼Œé‡‡æ ·åæ ‡ç‚¹
        for edge in root.findall(".//edge"):
            edge_id = edge.get("id")
            # è·³è¿‡å†…éƒ¨è¾¹
            if edge_id and edge_id.startswith(":"):
                continue
                
            for lane in edge.findall("lane"):
                shape = lane.get("shape")
                if shape:
                    # è§£æshapeå¹¶é‡‡æ ·
                    points = []
                    for point_str in shape.split():
                        x, y = map(float, point_str.split(','))
                        points.append((x, y))
                    
                    # é‡‡æ ·ä¸€å®šæ¯”ä¾‹çš„ç‚¹
                    num_samples = max(1, int(len(points) * sample_ratio))
                    step = max(1, len(points) // num_samples)
                    
                    for i in range(0, len(points), step):
                        sample_coords.append(points[i])
                    
                    # åªå¤„ç†ç¬¬ä¸€ä¸ªlane
                    break
        
        print(f"ä»ç½‘ç»œä¸­é‡‡æ ·äº† {len(sample_coords)} ä¸ªé“è·¯ç‚¹")
        return sample_coords
        
    except Exception as e:
        print(f"Warning: ä»ç½‘ç»œé‡‡æ ·é“è·¯ç‚¹å¤±è´¥: {e}")
        return []

def calculate_simple_coverage_features(cs_coords, lane_coords):
    """ç®€åŒ–ç‰ˆæœ¬çš„è¦†ç›–ç‰¹å¾è®¡ç®—"""
    try:
        # æŠ½æ ·ç­–ç•¥ï¼šé€‰æ‹©10%çš„ä»£è¡¨æ€§è·¯æ®µä½œä¸ºè™šæ‹Ÿéœ€æ±‚ç‚¹
        all_edges = list(lane_coords.keys())
        sample_size = max(1, int(len(all_edges) * 0.1))  # 10%æŠ½æ ·
        
        # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
        np.random.seed(42)
        sampled_edges = np.random.choice(all_edges, sample_size, replace=False)
        
        # è®¡ç®—æ¯ä¸ªé‡‡æ ·è·¯æ®µåˆ°æœ€è¿‘å……ç”µæ¡©çš„è·ç¦»
        distances_to_cs = []
        for edge_id in sampled_edges:
            edge_coord = lane_coords[edge_id]
            # è®¡ç®—åˆ°æ‰€æœ‰å……ç”µæ¡©çš„è·ç¦»ï¼Œå–æœ€å°å€¼
            dists = [np.linalg.norm(np.array(edge_coord) - np.array(cs_coord)) 
                    for cs_coord in cs_coords]
            min_dist = min(dists)
            distances_to_cs.append(min_dist)
        
        distances_to_cs = np.array(distances_to_cs)
        
        # 1. Coverage ratio (500må†…å¯åˆ°è¾¾å……ç”µæ¡©çš„è·¯æ®µæ¯”ä¾‹)
        coverage_ratio = np.mean(distances_to_cs <= 500)
        
        # 2. Max gap distance (æœ€å¤§æœåŠ¡ç©ºç™½è·ç¦»)
        max_gap_distance = np.max(distances_to_cs)
        
        # 3. Gini coefficient for service accessibility
        gini_coefficient = calculate_gini_coefficient(distances_to_cs)
        
        return coverage_ratio, max_gap_distance, gini_coefficient
        
    except Exception as e:
        print(f"Warning: è®¡ç®—è¦†ç›–ç‰¹å¾æ—¶å‡ºé”™: {e}")
        return 0.0, 0.0, 0.0

def calculate_gini_coefficient(values):
    """è®¡ç®—åŸºå°¼ç³»æ•°"""
    try:
        if len(values) == 0:
            return 0.0
        
        # æ’åº
        sorted_values = np.sort(values)
        n = len(sorted_values)
        
        # è®¡ç®—åŸºå°¼ç³»æ•°
        cumsum = np.cumsum(sorted_values)
        return (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n if cumsum[-1] > 0 else 0.0
        
    except:
        return 0.0

# å…¨å±€ç¼“å­˜ç½‘ç»œå›¾å’Œä¸­å¿ƒæ€§
_cached_network = None
_cached_centrality = None

def calculate_network_centrality(cs_coords, lane_coords, net_xml_path):
    """è®¡ç®—ç½‘ç»œä¸­å¿ƒæ€§æŒ‡æ ‡ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–æ€§èƒ½ï¼‰"""
    global _cached_network, _cached_centrality
    
    try:
        # ä½¿ç”¨ç¼“å­˜çš„ç½‘ç»œå›¾
        if _cached_network is None:
            print("ğŸ”§ é¦–æ¬¡æ„å»ºè·¯ç½‘å›¾ï¼ˆè¾ƒæ…¢ï¼‰...")
            _cached_network = build_road_network(net_xml_path)
            if _cached_network is None:
                print("Warning: æ— æ³•æ„å»ºæœ‰æ•ˆçš„è·¯ç½‘å›¾")
                return 0.0
        
        G = _cached_network
        
        # ä½¿ç”¨ç¼“å­˜çš„ä¸­å¿ƒæ€§è®¡ç®—
        if _cached_centrality is None:
            print("ğŸ”§ é¦–æ¬¡è®¡ç®—ä¸­å¿ƒæ€§ï¼ˆè¾ƒæ…¢ï¼‰...")
            try:
                # ä½¿ç”¨åˆç†çš„é‡‡æ ·æ•°é‡ä¿è¯ç²¾åº¦
                if len(G.nodes()) > 5000:
                    # é‡‡æ ·1000ä¸ªèŠ‚ç‚¹ä¿è¯è®¡ç®—ç²¾åº¦
                    _cached_centrality = nx.betweenness_centrality(G, k=min(1000, len(G.nodes())), normalized=True)
                else:
                    _cached_centrality = nx.betweenness_centrality(G, normalized=True)
                print("âœ… ä¸­å¿ƒæ€§è®¡ç®—å®Œæˆï¼Œåç»­åœºæ™¯å°†ä½¿ç”¨ç¼“å­˜")
            except Exception as e:
                print(f"Warning: ä¸­å¿ƒæ€§è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ³•: {e}")
                return calculate_simple_centrality(cs_coords, G)
        
        centrality = _cached_centrality
        
        # ä¸ºå……ç”µæ¡©æ‰¾åˆ°æœ€è¿‘çš„è·¯ç½‘èŠ‚ç‚¹
        cs_nodes = []
        for cs_coord in cs_coords:
            closest_node = find_closest_network_node(G, cs_coord)
            if closest_node is not None:
                cs_nodes.append(closest_node)
        
        if not cs_nodes:
            return 0.0
        
        # è®¡ç®—å……ç”µæ¡©ä½ç½®çš„å¹³å‡ä»‹æ•°ä¸­å¿ƒæ€§
        cs_centralities = [centrality.get(node, 0.0) for node in cs_nodes]
        avg_centrality = np.mean(cs_centralities) if cs_centralities else 0.0
        
        return avg_centrality
    
    except Exception as e:
        print(f"Warning: ç½‘ç»œä¸­å¿ƒæ€§è®¡ç®—å¤±è´¥: {e}")
        return 0.0

def calculate_simple_centrality(cs_coords, G):
    """ç®€åŒ–çš„ä¸­å¿ƒæ€§è®¡ç®—ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    try:
        # ç®€å•åŸºäºåº¦ä¸­å¿ƒæ€§
        degree_centrality = nx.degree_centrality(G)
        
        cs_nodes = []
        for cs_coord in cs_coords:
            closest_node = find_closest_network_node(G, cs_coord)
            if closest_node is not None:
                cs_nodes.append(closest_node)
        
        if not cs_nodes:
            return 0.0
        
        cs_centralities = [degree_centrality.get(node, 0.0) for node in cs_nodes]
        return np.mean(cs_centralities) if cs_centralities else 0.0
        
    except:
        return 0.0

def build_road_network(net_xml_path):
    """ä»net.xmlæ„å»ºè·¯ç½‘å›¾"""
    try:
        G = nx.Graph()
        tree = ET.parse(net_xml_path)
        root = tree.getroot()
        
        # æ·»åŠ èŠ‚ç‚¹
        for junction in root.findall(".//junction"):
            junction_id = junction.get("id")
            x = float(junction.get("x", 0))
            y = float(junction.get("y", 0))
            G.add_node(junction_id, pos=(x, y))
        
        # æ·»åŠ è¾¹ï¼ˆåŸºäºedgeè¿æ¥ï¼‰
        for edge in root.findall(".//edge"):
            edge_id = edge.get("id")
            from_node = edge.get("from")
            to_node = edge.get("to")
            
            # è·³è¿‡å†…éƒ¨è¾¹
            if edge_id and edge_id.startswith(":"):
                continue
                
            if from_node and to_node and from_node in G.nodes() and to_node in G.nodes():
                # è®¡ç®—è¾¹çš„é•¿åº¦ä½œä¸ºæƒé‡
                from_pos = G.nodes[from_node]["pos"]
                to_pos = G.nodes[to_node]["pos"]
                length = np.linalg.norm(np.array(from_pos) - np.array(to_pos))
                G.add_edge(from_node, to_node, weight=length, edge_id=edge_id)
        
        print(f"è·¯ç½‘å›¾æ„å»ºå®Œæˆ: {len(G.nodes())} ä¸ªèŠ‚ç‚¹, {len(G.edges())} æ¡è¾¹")
        return G
        
    except Exception as e:
        print(f"Warning: è·¯ç½‘æ„å»ºå¤±è´¥: {e}")
        return None

def find_closest_network_node(G, target_coord):
    """æ‰¾åˆ°è·ç¦»ç›®æ ‡åæ ‡æœ€è¿‘çš„ç½‘ç»œèŠ‚ç‚¹"""
    try:
        min_distance = float('inf')
        closest_node = None
        
        target_pos = np.array(target_coord)
        
        for node_id, data in G.nodes(data=True):
            if "pos" in data:
                node_pos = np.array(data["pos"])
                distance = np.linalg.norm(target_pos - node_pos)
                
                if distance < min_distance:
                    min_distance = distance
                    closest_node = node_id
        
        return closest_node
    
    except Exception as e:
        print(f"Warning: å¯»æ‰¾æœ€è¿‘èŠ‚ç‚¹å¤±è´¥: {e}")
        return None

def extract_all_layout_features_from_json(json_file_path, net_xml_path, output_dir=None):
    """ä»JSONæ–‡ä»¶æ‰¹é‡æå–æ‰€æœ‰å……ç”µæ¡©å¸ƒå±€çš„ç‰¹å¾"""
    if output_dir is None:
        output_dir = os.path.dirname(json_file_path)
    
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file_path, 'r', encoding='utf-8') as f:
            cs_data = json.load(f)
        
        print(f"ğŸ“Š åŠ è½½äº† {len(cs_data)} ä¸ªå……ç”µæ¡©å¸ƒå±€ç»„")
        
        # æ£€æŸ¥å·²å®Œæˆçš„æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        completed_groups = set()
        final_output_file = os.path.join(output_dir, "all_layout_features.csv")
        if os.path.exists(final_output_file):
            try:
                existing_df = pd.read_csv(final_output_file)
                completed_groups = set(existing_df['layout_id'].tolist())
                print(f"ğŸ”„ å‘ç°å·²å®Œæˆçš„å¸ƒå±€ç»„: {len(completed_groups)} ä¸ª")
            except:
                pass
        
        all_features = []
        success_count = 0
        total_count = len(cs_data)
        skipped_count = 0
        
        for idx, (group_id, group_data) in enumerate(cs_data.items(), 1):
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
            if group_id in completed_groups:
                skipped_count += 1
                print(f"\n[{idx}/{total_count}] â­ï¸ è·³è¿‡å·²å®Œæˆ: {group_id}")
                continue
                
            print(f"\n[{idx}/{total_count}] å¤„ç†å¸ƒå±€ç»„: {group_id}")
            
            try:
                features = extract_layout_features_from_json(group_data, net_xml_path)
                features["layout_id"] = group_id
                all_features.append(features)
                success_count += 1
                print(f"âœ… æå–ç‰¹å¾å®Œæˆ: {group_id} ({len(group_data)} ä¸ªå……ç”µæ¡©)")
                
                # ç«‹å³ä¿å­˜å•ä¸ªç»“æœ
                single_df = pd.DataFrame([features])
                single_output_file = os.path.join(output_dir, f"{group_id}_layout_features.csv")
                single_df.to_csv(single_output_file, index=False)
                print(f"ğŸ’¾ å•ä¸ªæ–‡ä»¶å·²ä¿å­˜: {single_output_file}")
                
                # ç«‹å³æ›´æ–°æ±‡æ€»æ–‡ä»¶
                if os.path.exists(final_output_file):
                    # è¯»å–ç°æœ‰æ–‡ä»¶å¹¶è¿½åŠ 
                    existing_df = pd.read_csv(final_output_file)
                    updated_df = pd.concat([existing_df, single_df], ignore_index=True)
                    updated_df.to_csv(final_output_file, index=False)
                else:
                    # åˆ›å»ºæ–°æ–‡ä»¶
                    single_df.to_csv(final_output_file, index=False)
                print(f"ğŸ’¾ æ±‡æ€»æ–‡ä»¶å·²æ›´æ–°: {final_output_file}")
                
                # æ¯å¤„ç†5ä¸ªå°±å¤‡ä»½ä¸€æ¬¡
                if success_count % 5 == 0:
                    backup_file = os.path.join(output_dir, f"all_layout_features_backup_{success_count}.csv")
                    if os.path.exists(final_output_file):
                        import shutil
                        shutil.copy2(final_output_file, backup_file)
                        print(f"ğŸ”„ å¤‡ä»½æ–‡ä»¶: {backup_file}")
                
            except Exception as e:
                print(f"âŒ æå–ç‰¹å¾å¤±è´¥ {group_id}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        if all_features:
            # ä¿å­˜æ‰€æœ‰ç‰¹å¾åˆ°ä¸€ä¸ªæ–‡ä»¶
            df = pd.DataFrame(all_features)
            output_file = os.path.join(output_dir, "all_layout_features.csv")
            df.to_csv(output_file, index=False)
            print(f"\nâœ… æ‰€æœ‰ç‰¹å¾ä¿å­˜åˆ°: {output_file}")
            
            # åŒæ—¶ä¸ºæ¯ä¸ªå¸ƒå±€ç”Ÿæˆå•ç‹¬çš„ç‰¹å¾æ–‡ä»¶
            for features in all_features:
                layout_id = features["layout_id"]
                single_df = pd.DataFrame([features])
                single_output_file = os.path.join(output_dir, f"{layout_id}_layout_features.csv")
                single_df.to_csv(single_output_file, index=False)
            
            print(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
            print(f"âœ… æˆåŠŸå¤„ç†: {success_count} ä¸ª")
            print(f"â­ï¸ è·³è¿‡å·²å®Œæˆ: {skipped_count} ä¸ª") 
            print(f"ğŸ“Š æ€»è®¡: {success_count + skipped_count}/{len(cs_data)} ä¸ªå¸ƒå±€ç»„")
            
        return all_features
        
    except Exception as e:
        print(f"âŒ å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {e}")
        return []

def extract_all_layout_features(cs_dir, net_xml_path, output_dir=None):
    """æ‰¹é‡æå–æ‰€æœ‰å……ç”µæ¡©å¸ƒå±€çš„ç‰¹å¾ï¼ˆXMLæ–‡ä»¶ç‰ˆæœ¬ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰"""
    if output_dir is None:
        output_dir = cs_dir
    
    # è·å–æ‰€æœ‰cs_group_*.xmlæ–‡ä»¶
    cs_files = []
    for file in os.listdir(cs_dir):
        if file.startswith("cs_group_") and file.endswith(".xml"):
            cs_files.append(file)
    
    cs_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
    
    all_features = []
    
    for cs_file in cs_files:
        cs_xml_path = os.path.join(cs_dir, cs_file)
        layout_id = cs_file.replace(".xml", "")
        
        try:
            features = extract_layout_features(cs_xml_path, net_xml_path)
            features["layout_id"] = layout_id
            all_features.append(features)
            print(f"âœ… æå–ç‰¹å¾å®Œæˆ: {layout_id}")
        except Exception as e:
            print(f"âŒ æå–ç‰¹å¾å¤±è´¥ {layout_id}: {e}")
            continue
    
    # ä¿å­˜åˆ°CSVæ–‡ä»¶
    if all_features:
        df = pd.DataFrame(all_features)
        output_file = os.path.join(output_dir, "all_layout_features.csv")
        df.to_csv(output_file, index=False)
        print(f"âœ… æ‰€æœ‰ç‰¹å¾ä¿å­˜åˆ°: {output_file}")
        
        # åŒæ—¶ä¸ºæ¯ä¸ªå¸ƒå±€ç”Ÿæˆå•ç‹¬çš„ç‰¹å¾æ–‡ä»¶
        for features in all_features:
            layout_id = features["layout_id"]
            single_df = pd.DataFrame([features])
            single_output_file = os.path.join(output_dir, f"{layout_id}_layout_features.csv")
            single_df.to_csv(single_output_file, index=False)
    
    return all_features

def main():
    parser = argparse.ArgumentParser(description='æå–å……ç”µæ¡©å¸ƒå±€ç‰¹å¾')
    parser.add_argument('--layout_file', type=str, 
                       help='å¸ƒå±€æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼ï¼Œå¦‚ cs_candidates.json)')
    parser.add_argument('--net_file', type=str,
                       default='data/map/glasgow_clean.net.xml',
                       help='ç½‘ç»œæ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/map/glasgow_clean.net.xml)')
    parser.add_argument('--output_dir', type=str,
                       help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: å¸ƒå±€æ–‡ä»¶æ‰€åœ¨ç›®å½•)')
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æä¾›å¸ƒå±€æ–‡ä»¶å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼å¹¶æ˜¾ç¤ºå¸®åŠ©
    if not args.layout_file:
        print("ğŸš€ å¼€å§‹å¤„ç†å……ç”µæ¡©å¸ƒå±€ç‰¹å¾æå–")
        print("ğŸ’¡ æœªæŒ‡å®šå¸ƒå±€æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # é»˜è®¤é…ç½®
        json_file_path = "/home/ubuntu/project/MSC/Msc_Project/data/cs/cs_candidates.json"
        net_xml_path = "/home/ubuntu/project/MSC/Msc_Project/data/map/glasgow_clean.net.xml"
        output_dir = "/home/ubuntu/project/MSC/Msc_Project/data/cs"
    else:
        # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        json_file_path = args.layout_file
        net_xml_path = args.net_file
        output_dir = args.output_dir if args.output_dir else os.path.dirname(args.layout_file)
        
        print("ğŸš€ å¼€å§‹å¤„ç†å……ç”µæ¡©å¸ƒå±€ç‰¹å¾æå–")
        print(f"ğŸ“Š ä½¿ç”¨æŒ‡å®šçš„å¸ƒå±€æ–‡ä»¶: {json_file_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(json_file_path):
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
        parser.print_help()
        print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print("   # ä½¿ç”¨é»˜è®¤é…ç½®")
        print("   python scripts/extract_layout_features.py")
        print("\n   # æŒ‡å®šå¸ƒå±€æ–‡ä»¶")
        print("   python scripts/extract_layout_features.py --layout_file data/cs_51-100/cs_candidates_51-100.json")
        print("\n   # æŒ‡å®šå¸ƒå±€æ–‡ä»¶å’Œç½‘ç»œæ–‡ä»¶")
        print("   python scripts/extract_layout_features.py --layout_file data/cs_51-100/cs_candidates_51-100.json --net_file data/map/glasgow_clean.net.xml")
        print("\n   # æŒ‡å®šè¾“å‡ºç›®å½•")
        print("   python scripts/extract_layout_features.py --layout_file data/cs_51-100/cs_candidates_51-100.json --output_dir output/features")
        exit(1)
    
    if not os.path.exists(net_xml_path):
        print(f"âŒ ç½‘ç»œæ–‡ä»¶ä¸å­˜åœ¨: {net_xml_path}")
        exit(1)
    
    print(f"ğŸ“Š JSONæ–‡ä»¶: {json_file_path}")
    print(f"ğŸ—ºï¸ ç½‘ç»œæ–‡ä»¶: {net_xml_path}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æ‰§è¡Œæ‰¹é‡ç‰¹å¾æå–
    try:
        all_features = extract_all_layout_features_from_json(
            json_file_path=json_file_path,
            net_xml_path=net_xml_path,
            output_dir=output_dir
        )
        
        if all_features:
            print(f"\nğŸ‰ ç‰¹å¾æå–å®Œæˆï¼å…±å¤„ç† {len(all_features)} ä¸ªå¸ƒå±€ç»„")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·ä¾‹çš„ç‰¹å¾
            if len(all_features) > 0:
                print(f"\nğŸ“‹ æ ·ä¾‹ç‰¹å¾ ({all_features[0]['layout_id']}):")
                for k, v in all_features[0].items():
                    if k != 'layout_id':
                        print(f"  {k}: {v:.4f}" if isinstance(v, float) else f"  {k}: {v}")
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾")
            
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

