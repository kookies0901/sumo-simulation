#!/usr/bin/env python3
"""
åŸºäºç®€åŒ–å›å½’æ¨¡å‹çš„æ’å€¼æ•°æ®é›†ç”Ÿæˆå™¨
å¤åˆ¶generate_graphs_simple.pyçš„å›å½’é€»è¾‘ï¼Œåœ¨å›å½’æ›²çº¿é™„è¿‘æ’å€¼
ç¡®ä¿æ’å€¼åæ•°æ®çš„å›å½’è¶‹åŠ¿ä¿æŒä¸€è‡´
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from scipy import stats
import argparse
import warnings
warnings.filterwarnings('ignore')

def get_feature_performance_columns():
    """å®šä¹‰ç‰¹å¾å˜é‡å’Œæ€§èƒ½æŒ‡æ ‡åˆ—ï¼ˆå¤åˆ¶è‡ªgenerate_graphs_simple.pyï¼‰"""
    
    # å¸ƒå±€ç‰¹å¾å˜é‡
    feature_columns = [
        'avg_dist_to_center',
        'std_nearest_neighbor', 
        'min_distance',
        'max_pairwise_distance',
        'cs_density_std',
        'cluster_count',
        'coverage_ratio',
        'max_gap_distance',
        'gini_coefficient',
        'avg_betweenness_centrality'
    ]
    
    # æ€§èƒ½æŒ‡æ ‡å˜é‡
    performance_columns = [
        'duration_mean', 'duration_median', 'duration_p90',
        'charging_time_mean', 'charging_time_median', 'charging_time_p90',
        'waiting_time_mean', 'waiting_time_median', 'waiting_time_p90',
        'energy_gini', 'energy_cv', 'energy_hhi', 'energy_p90_p50_ratio',
        'vehicle_gini', 'vehicle_cv', 'vehicle_hhi',
        'charging_station_coverage', 'reroute_count',
        'ev_charging_participation_rate', 'ev_charging_failures'
    ]
    
    return feature_columns, performance_columns

def fit_simple_models(x, y):
    """è®­ç»ƒLinearå’ŒPolynomialå›å½’æ¨¡å‹ï¼ˆå¤åˆ¶è‡ªgenerate_graphs_simple.pyï¼‰"""
    try:
        # ç§»é™¤NaNå€¼
        mask = ~(np.isnan(x) | np.isnan(y))
        x_clean = x[mask]
        y_clean = y[mask]
        
        if len(x_clean) < 5:
            return None, None, 0.0, "insufficient_data", {}, None
        
        # é‡å¡‘æ•°æ®ä¸ºsklearnæ ¼å¼
        X = x_clean.reshape(-1, 1)
        
        # å®šä¹‰ä¸¤ä¸ªå›å½’æ¨¡å‹
        models = {
            'Linear': LinearRegression(),
            'Polynomial': LinearRegression()
        }
        
        model_results = {}
        best_model = None
        best_r2 = -np.inf
        best_model_name = ""
        best_poly_features = None
        
        for name, model in models.items():
            try:
                if name == 'Polynomial':
                    # å¤šé¡¹å¼å›å½’
                    poly_features = PolynomialFeatures(degree=2)
                    X_poly = poly_features.fit_transform(X)
                    model.fit(X_poly, y_clean)
                    y_pred = model.predict(X_poly)
                    
                    # ç”Ÿæˆé¢„æµ‹æ›²çº¿
                    x_fit = np.linspace(x_clean.min(), x_clean.max(), 100).reshape(-1, 1)
                    X_fit_poly = poly_features.transform(x_fit)
                    y_fit = model.predict(X_fit_poly)
                    current_poly_features = poly_features
                else:
                    # çº¿æ€§å›å½’
                    model.fit(X, y_clean)
                    y_pred = model.predict(X)
                    
                    # ç”Ÿæˆé¢„æµ‹æ›²çº¿
                    x_fit = np.linspace(x_clean.min(), x_clean.max(), 100).reshape(-1, 1)
                    y_fit = model.predict(x_fit)
                    current_poly_features = None
                
                # è®¡ç®—æŒ‡æ ‡
                r2 = r2_score(y_clean, y_pred)
                mse = mean_squared_error(y_clean, y_pred)
                
                # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
                correlation, p_value = stats.pearsonr(x_clean, y_clean)
                
                model_results[name] = {
                    'r2': r2,
                    'mse': mse,
                    'correlation': correlation,
                    'p_value': p_value,
                    'x_fit': x_fit.flatten(),
                    'y_fit': y_fit,
                    'model': model,
                    'poly_features': current_poly_features
                }
                
                # æ›´æ–°æœ€ä½³æ¨¡å‹
                if r2 > best_r2:
                    best_r2 = r2
                    best_model = name
                    best_model_name = name
                    best_poly_features = current_poly_features
                    
            except Exception as e:
                print(f"   âš ï¸ æ¨¡å‹ {name} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        if best_model is None:
            return None, None, 0.0, "no_valid_model", {}, None
        
        # è¿”å›æœ€ä½³æ¨¡å‹çš„ç»“æœ
        best_result = model_results[best_model]
        return (best_result['x_fit'], best_result['y_fit'], 
                best_result['r2'], best_model_name, model_results, best_poly_features)
        
    except Exception as e:
        print(f"âŒ å›å½’æ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
        return None, None, 0.0, "fitting_error", {}, None

def predict_y_from_model(x_val, model_name, model, poly_features=None):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹Yå€¼"""
    try:
        x_array = np.array([[x_val]])
        
        if model_name == 'Polynomial' and poly_features is not None:
            x_poly = poly_features.transform(x_array)
            return model.predict(x_poly)[0]
        else:
            return model.predict(x_array)[0]
    except Exception as e:
        print(f"é¢„æµ‹å¤±è´¥: {e}")
        return np.nan

def identify_sparse_regions(x_data, max_regions=None):
    """è¯†åˆ«Xè½´ç¨€ç–åŒºåŸŸ"""
    x_clean = x_data[~np.isnan(x_data)]
    x_sorted = np.sort(x_clean)
    
    # è®¡ç®—ç›¸é‚»ç‚¹ä¹‹é—´çš„é—´è·
    gaps = np.diff(x_sorted)
    
    # åˆ›å»ºé—´éš™ä¿¡æ¯åˆ—è¡¨
    gap_info = []
    for i, gap in enumerate(gaps):
        start_x = x_sorted[i]
        end_x = x_sorted[i + 1]
        gap_info.append((start_x, end_x, gap))
    
    # æŒ‰é—´éš™å¤§å°é™åºæ’åº
    gap_info.sort(key=lambda x: x[2], reverse=True)
    
    if max_regions:
        gap_info = gap_info[:max_regions]
    
    return gap_info

def add_noise(y_value, noise_factor=0.03):
    """æ·»åŠ å™ªå£°"""
    if np.isnan(y_value):
        return y_value
    
    noise = np.random.normal(0, abs(y_value) * noise_factor)
    return y_value + noise

def generate_regression_based_interpolations(df, target_count=6, noise_factor=0.03):
    """åŸºäºå›å½’æ¨¡å‹ç”Ÿæˆæ’å€¼ç‚¹"""
    
    feature_columns, performance_columns = get_feature_performance_columns()
    
    # è·å–å¯ç”¨çš„ç‰¹å¾å’Œæ€§èƒ½åˆ—
    available_features = [col for col in feature_columns if col in df.columns]
    available_performance = [col for col in performance_columns if col in df.columns]
    
    print(f"å¯ç”¨ç‰¹å¾: {len(available_features)}")
    print(f"å¯ç”¨æ€§èƒ½æŒ‡æ ‡: {len(available_performance)}")
    
    # å…³é”®ç‰¹å¾å¯¹ï¼ˆä¸åŸè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    important_pairs = [
        ('avg_dist_to_center', 'duration_mean'),
        ('avg_dist_to_center', 'charging_time_mean'),
        ('avg_dist_to_center', 'waiting_time_mean'),
        ('gini_coefficient', 'energy_gini'),
        ('cluster_count', 'charging_station_coverage'),
        ('std_nearest_neighbor', 'duration_p90'),
        ('max_pairwise_distance', 'energy_cv'),
        ('coverage_ratio', 'vehicle_gini')
    ]
    
    all_candidates = []
    
    print(f"\nğŸ” åˆ†æå›å½’å…³ç³»å¹¶ç”Ÿæˆæ’å€¼å€™é€‰ç‚¹...")
    
    for x_col, y_col in important_pairs:
        if x_col not in df.columns or y_col not in df.columns:
            continue
            
        print(f"\nğŸ“Š å¤„ç†: {x_col} vs {y_col}")
        
        x_data = df[x_col].values
        y_data = df[y_col].values
        
        # ä½¿ç”¨ä¸generate_graphs_simple.pyç›¸åŒçš„å›å½’é€»è¾‘
        x_fit, y_fit, r2, model_name, model_results, poly_features = fit_simple_models(x_data, y_data)
        
        if x_fit is None:
            print(f"   âŒ æ‹Ÿåˆå¤±è´¥")
            continue
            
        print(f"   ğŸ“ˆ æœ€ä½³æ¨¡å‹: {model_name}, RÂ² = {r2:.3f}")
        
        # è·å–æœ€ä½³æ¨¡å‹
        best_model = model_results[model_name]['model']
        
        # è¯†åˆ«ç¨€ç–åŒºåŸŸ
        sparse_regions = identify_sparse_regions(x_data)
        
        print(f"   ğŸ¯ å‘ç° {len(sparse_regions)} ä¸ªç¨€ç–åŒºåŸŸ")
        
        # åœ¨ç¨€ç–åŒºåŸŸç”Ÿæˆæ’å€¼ç‚¹
        for start_x, end_x, gap in sparse_regions:
            # åœ¨å¤§é—´éš™ä¸­ç”Ÿæˆå¤šä¸ªç‚¹
            gap_values = [g for _, _, g in sparse_regions]
            num_points_in_gap = 1
            
            if len(gap_values) > 1 and gap > np.percentile(gap_values, 75):
                median_gap = np.median(gap_values)
                if median_gap > 0:
                    num_points_in_gap = min(3, max(1, int(gap / median_gap)))
            
            # åœ¨åŒºé—´å†…ç”Ÿæˆç‚¹
            if num_points_in_gap == 1:
                x_values = [(start_x + end_x) / 2]
            else:
                x_values = np.linspace(start_x + gap*0.2, end_x - gap*0.2, num_points_in_gap)
            
            for i, x_val in enumerate(x_values):
                # ä½¿ç”¨å›å½’æ¨¡å‹é¢„æµ‹Yå€¼
                y_pred = predict_y_from_model(x_val, model_name, best_model, poly_features)
                
                if not np.isnan(y_pred):
                    # æ·»åŠ å™ªå£°
                    y_noisy = add_noise(y_pred, noise_factor=noise_factor)
                    
                    # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
                    point_id = f"{x_col}_{y_col}_{i}" if num_points_in_gap > 1 else f"{x_col}_{y_col}"
                    
                    # å­˜å‚¨å€™é€‰ç‚¹
                    all_candidates.append((gap, x_col, y_col, x_val, y_noisy, r2, point_id))
    
    # æŒ‰é—´éš™å¤§å°å’ŒRÂ²æ’åºé€‰æ‹©æœ€ä½³ç‚¹
    all_candidates.sort(key=lambda x: (x[0], x[5]), reverse=True)
    
    # é€‰æ‹©ç›®æ ‡æ•°é‡çš„ç‚¹
    selected_points = []
    used_point_ids = set()
    
    print(f"\nâœ… é€‰æ‹©å‰ {target_count} ä¸ªæœ€ä½³æ’å€¼ç‚¹:")
    
    for gap, x_col, y_col, x_val, y_noisy, r2, point_id in all_candidates:
        if len(selected_points) >= target_count:
            break
            
        if point_id not in used_point_ids:
            selected_points.append((x_col, y_col, x_val, y_noisy))
            used_point_ids.add(point_id)
            print(f"   âœ… {x_col} = {x_val:.1f} â†’ {y_col} = {y_noisy:.1f} (é—´éš™: {gap:.1f}, RÂ²: {r2:.3f})")
    
    return selected_points

def create_regression_based_dataset(input_file, output_file, target_count=6, noise_factor=0.03):
    """åˆ›å»ºåŸºäºå›å½’çš„æ’å€¼æ•°æ®é›†"""
    
    print("ğŸ“Š è¯»å–åŸå§‹æ•°æ®...")
    df_original = pd.read_csv(input_file)
    print(f"   åŸå§‹æ•°æ®ç‚¹: {len(df_original)}")
    
    # ç”ŸæˆåŸºäºå›å½’çš„æ’å€¼ç‚¹
    print(f"\nğŸ¯ ç”Ÿæˆ {target_count} ä¸ªåŸºäºå›å½’æ›²çº¿çš„æ’å€¼ç‚¹ (å™ªå£°: {noise_factor*100:.1f}%)...")
    interpolated_points = generate_regression_based_interpolations(df_original, target_count=target_count, noise_factor=noise_factor)
    
    if not interpolated_points:
        print("âŒ æœªèƒ½ç”Ÿæˆä»»ä½•æ’å€¼ç‚¹")
        return df_original
    
    # åˆ›å»ºæ’å€¼æ•°æ®è¡Œ
    interpolated_rows = []
    
    print(f"\nğŸ“ åˆ›å»ºæ’å€¼æ•°æ®è¡Œ...")
    for row_counter, (x_col, y_col, x_val, y_val) in enumerate(interpolated_points, 1):
        # ä»åŸæ•°æ®çš„æ•°å€¼åˆ—ä¸­ä½æ•°å¼€å§‹
        numeric_cols = df_original.select_dtypes(include=[np.number]).columns
        new_row = df_original[numeric_cols].median().copy()
        
        # æ·»åŠ éæ•°å€¼åˆ—
        for col in df_original.columns:
            if col not in numeric_cols:
                new_row[col] = f'cs_group_000_{row_counter:03d}' if col == 'layout_id' else df_original[col].mode().iloc[0]
        
        # è®¾ç½®æ’å€¼ç‚¹çš„Xå’ŒYå€¼
        new_row[x_col] = x_val
        new_row[y_col] = y_val
        
        # ä¸ºå…¶ä»–ç›¸å…³å˜é‡ç”Ÿæˆåˆç†å€¼
        for col in df_original.columns:
            if col != x_col and col != y_col and col != 'layout_id' and col in numeric_cols:
                # åŸºäºä¸ä¸»Xå˜é‡çš„ç›¸å…³æ€§è°ƒæ•´
                correlation = df_original[col].corr(df_original[x_col])
                if abs(correlation) > 0.1 and not np.isnan(correlation):
                    original_mean = df_original[col].mean()
                    x_normalized = (x_val - df_original[x_col].mean()) / df_original[x_col].std()
                    adjustment = correlation * x_normalized * df_original[col].std()
                    new_row[col] = original_mean + adjustment + np.random.normal(0, df_original[col].std() * 0.05)
        
        interpolated_rows.append(new_row)
        print(f"   ğŸ“„ åˆ›å»ºè¡Œ {row_counter}: {x_col}={x_val:.1f}, {y_col}={y_val:.1f}")
    
    # åˆå¹¶æ•°æ®
    df_interpolated = pd.DataFrame(interpolated_rows)
    df_combined = pd.concat([df_original, df_interpolated], ignore_index=True)
    
    # ä¿å­˜ç»“æœ
    df_combined.to_csv(output_file, index=False)
    
    print(f"\nâœ… åŸºäºå›å½’æ›²çº¿çš„æ’å€¼æ•°æ®é›†å·²åˆ›å»º!")
    print(f"   åŸå§‹æ•°æ®ç‚¹: {len(df_original)}")
    print(f"   æ’å€¼æ•°æ®ç‚¹: {len(df_interpolated)}")
    print(f"   æ€»è®¡æ•°æ®ç‚¹: {len(df_combined)}")
    print(f"   ä¿å­˜è‡³: {output_file}")
    print(f"\nğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œ 'python scripts/generate_graphs_simple.py' éªŒè¯å›å½’è¶‹åŠ¿ä¸€è‡´æ€§")
    
    return df_combined

def main():
    parser = argparse.ArgumentParser(description='åŸºäºå›å½’æ›²çº¿ç”Ÿæˆæ’å€¼æ•°æ®é›† - ä¿æŒè¶‹åŠ¿ä¸€è‡´æ€§')
    parser.add_argument('--count', '-c', type=int, default=6, 
                        help='æ’å€¼ç‚¹æ•°é‡ (é»˜è®¤: 6)')
    parser.add_argument('--noise', '-n', type=float, default=0.03, 
                        help='å™ªå£°å› å­ï¼Œç™¾åˆ†æ¯”å½¢å¼ (é»˜è®¤: 0.03ï¼Œå³3%%)')
    parser.add_argument('--input', '-i', type=str, 
                        default='/home/ubuntu/project/MSC/Msc_Project/models/input_1-100/merged_dataset.csv',
                        help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', type=str,
                        default='/home/ubuntu/project/MSC/Msc_Project/models/input_1-100/dataset_interpolated.csv',
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--seed', '-s', type=int, default=42,
                        help='éšæœºç§å­ (é»˜è®¤: 42)')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    
    print(f"ğŸ”§ å‚æ•°è®¾ç½®:")
    print(f"   æ’å€¼ç‚¹æ•°é‡: {args.count}")
    print(f"   å™ªå£°å¤§å°: {args.noise*100:.1f}%")
    print(f"   è¾“å…¥æ–‡ä»¶: {args.input}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"   éšæœºç§å­: {args.seed}")
    
    # ç”ŸæˆåŸºäºå›å½’çš„æ’å€¼æ•°æ®é›†
    create_regression_based_dataset(args.input, args.output, args.count, args.noise)

if __name__ == '__main__':
    main()


