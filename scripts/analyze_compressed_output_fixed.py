#!/usr/bin/env python3
"""
åˆ†æSUMOè¾“å‡ºæ–‡ä»¶ - ä¿®å¤æµ®ç‚¹ç²¾åº¦é—®é¢˜ç‰ˆæœ¬
è§£å†³duration_p90å’Œwaiting_time_p90çš„å°¾æ•°ç²¾åº¦é—®é¢˜
"""

import os
import sys
import csv
import json
import argparse
import xml.etree.ElementTree as ET
import pandas as pd
import logging
import gzip
import shutil
import numpy as np
from datetime import datetime
from decimal import Decimal, ROUND_HALF_UP

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )

def safe_float_convert(value_str, default=0.0, precision=6):
    """å®‰å…¨çš„æµ®ç‚¹æ•°è½¬æ¢ï¼Œé¿å…ç²¾åº¦é—®é¢˜"""
    try:
        # å…ˆè½¬ä¸ºå­—ç¬¦ä¸²ï¼Œå†ç”¨Decimalè¿›è¡Œç²¾ç¡®è®¡ç®—
        if isinstance(value_str, str):
            # ç§»é™¤å¯èƒ½çš„ç§‘å­¦è®¡æ•°æ³•å’Œå¤šä½™ç²¾åº¦
            decimal_val = Decimal(value_str)
        else:
            decimal_val = Decimal(str(value_str))
        
        # å››èˆäº”å…¥åˆ°æŒ‡å®šç²¾åº¦
        rounded_decimal = decimal_val.quantize(
            Decimal('0.' + '0' * precision), 
            rounding=ROUND_HALF_UP
        )
        
        return float(rounded_decimal)
    except:
        return default

def calculate_statistics_fixed(values, name="æ•°æ®", precision=4):
    """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ï¼šmean, median, p90 - ä¿®å¤ç²¾åº¦é—®é¢˜ç‰ˆæœ¬"""
    if not values:
        return {"mean": 0.0, "median": 0.0, "p90": 0.0}
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶æ¸…ç†ç²¾åº¦
    clean_values = []
    for v in values:
        if isinstance(v, (int, float)) and not np.isnan(v) and not np.isinf(v):
            # ä½¿ç”¨roundè€Œä¸æ˜¯Decimalä»¥ä¿æŒå…¼å®¹æ€§ï¼Œä½†é™åˆ¶ç²¾åº¦
            clean_val = round(float(v), precision)
            clean_values.append(clean_val)
    
    if not clean_values:
        return {"mean": 0.0, "median": 0.0, "p90": 0.0}
    
    values_array = np.array(clean_values)
    
    # è®¡ç®—ç»Ÿè®¡é‡å¹¶å››èˆäº”å…¥
    mean = round(float(np.mean(values_array)), precision)
    median = round(float(np.median(values_array)), precision)
    p90 = round(float(np.percentile(values_array, 90)), precision)
    
    logging.info(f"ğŸ“Š {name}ç»Ÿè®¡: mean={mean:.2f}, median={median:.2f}, p90={p90:.2f}")
    return {"mean": mean, "median": median, "p90": p90}

def calculate_gini_coefficient(values):
    """è®¡ç®—åŸºå°¼ç³»æ•°"""
    if not values or len(values) == 0:
        return 0.0
    
    values = np.array(values)
    if np.sum(values) == 0:
        return 0.0
    
    # æ’åºå¹¶è®¡ç®—ç´¯ç§¯ä»½é¢
    sorted_values = np.sort(values)
    n = len(sorted_values)
    cumsum = np.cumsum(sorted_values)
    
    # è®¡ç®—åŸºå°¼ç³»æ•°
    gini = (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n
    return round(gini, 6)

def calculate_hhi(values):
    """è®¡ç®—HHIæŒ‡æ•° (Herfindahl-Hirschman Index)"""
    if not values or len(values) == 0:
        return 0.0
    
    values = np.array(values)
    total = np.sum(values)
    if total == 0:
        return 0.0
    
    # è®¡ç®—å¸‚åœºä»½é¢çš„å¹³æ–¹å’Œ
    market_shares = values / total
    hhi = np.sum(market_shares ** 2)
    return round(hhi, 6)

def calculate_cv(values):
    """è®¡ç®—å˜å¼‚ç³»æ•° (Coefficient of Variation)"""
    if not values or len(values) == 0:
        return 0.0
    
    values = np.array(values)
    mean = np.mean(values)
    if mean == 0:
        return 0.0
    
    std = np.std(values)
    cv = std / mean
    return round(cv, 6)

def parse_tripinfo_data_fixed(xml_file_path):
    """è§£ætripinfoæ•°æ®ï¼Œä¿®å¤ç²¾åº¦é—®é¢˜"""
    logging.info(f"ğŸ” å¼€å§‹è§£ætripinfoæ•°æ®: {xml_file_path}")
    
    data = {
        'durations': [],
        'waiting_times': [],
        'reroute_count': 0,
        'ev_charging_failures': 0,
        'total_vehicles': 0
    }
    
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()
        
        for tripinfo in root.findall("tripinfo"):
            data['total_vehicles'] += 1
            
            # è·å–duration - ä½¿ç”¨å®‰å…¨è½¬æ¢
            duration_str = tripinfo.get("duration", "0")
            duration = safe_float_convert(duration_str, 0.0)
            data['durations'].append(duration)
            
            # è·å–waitingTime - ä½¿ç”¨å®‰å…¨è½¬æ¢
            waiting_time_str = tripinfo.get("waitingTime", "0")
            waiting_time = safe_float_convert(waiting_time_str, 0.0)
            data['waiting_times'].append(waiting_time)
            
            # ç»Ÿè®¡rerouteæ¬¡æ•°
            reroute_no = int(tripinfo.get("rerouteNo", 0))
            if reroute_no > 0:
                data['reroute_count'] += 1
        
        # ç»Ÿè®¡stationfinderå…ƒç´ æ•°é‡ï¼ˆå……ç”µå¤±è´¥çš„EVï¼‰
        stationfinder_count = 0
        for tripinfo in root.findall("tripinfo"):
            stationfinders = tripinfo.findall("stationfinder")
            stationfinder_count += len(stationfinders)
        data['ev_charging_failures'] = stationfinder_count
        
        logging.info(f"ğŸ“Š è§£æå®Œæˆ: {data['total_vehicles']} è¾†è½¦")
        logging.info(f"ğŸ“Š é‡æ–°è·¯ç”±è½¦è¾†æ•°: {data['reroute_count']}")
        logging.info(f"ğŸ“Š EVå……ç”µå¤±è´¥æ•°: {data['ev_charging_failures']}")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        duration_info = f"DurationèŒƒå›´: {min(data['durations']):.2f} - {max(data['durations']):.2f}"
        waiting_info = f"WaitingTimeèŒƒå›´: {min(data['waiting_times']):.2f} - {max(data['waiting_times']):.2f}"
        logging.info(f"ğŸ“Š {duration_info}")
        logging.info(f"ğŸ“Š {waiting_info}")
        
    except Exception as e:
        logging.error(f"âŒ è§£ætripinfoæ•°æ®å¤±è´¥: {e}")
    
    return data

def parse_charging_events_data_fixed(xml_file_path):
    """è§£æchargingeventsæ•°æ®ï¼Œä¿®å¤ç²¾åº¦é—®é¢˜"""
    logging.info(f"ğŸ” å¼€å§‹è§£æchargingeventsæ•°æ®: {xml_file_path}")
    
    data = {
        'charging_steps': [],
        'total_energy_charged': [],
        'charging_vehicles_count': [],
        'ev_charging_participation': set(),
        'total_charging_stations': 0,
        'used_charging_stations': 0
    }
    
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()
        
        for station in root.findall("chargingStation"):
            data['total_charging_stations'] += 1
            
            # è·å–å……ç”µæ—¶é—´ - ä½¿ç”¨å®‰å…¨è½¬æ¢
            charging_steps_str = station.get("chargingSteps", "0")
            charging_steps = safe_float_convert(charging_steps_str, 0.0)
            if charging_steps > 0:
                data['charging_steps'].append(charging_steps)
            
            # è·å–æ€»å……ç”µé‡ - ä½¿ç”¨å®‰å…¨è½¬æ¢
            total_energy_str = station.get("totalEnergyCharged", "0")
            total_energy = safe_float_convert(total_energy_str, 0.0)
            data['total_energy_charged'].append(total_energy)
            
            if total_energy > 0:
                data['used_charging_stations'] += 1
            
            # ç»Ÿè®¡å……ç”µè½¦è¾†æ•°
            vehicle_count = len(station.findall("vehicle"))
            data['charging_vehicles_count'].append(vehicle_count)
            
            # ç»Ÿè®¡å‚ä¸å……ç”µçš„EV
            for vehicle in station.findall("vehicle"):
                veh_id = vehicle.get("id", "")
                if veh_id.startswith("EV_"):
                    data['ev_charging_participation'].add(veh_id)
        
        logging.info(f"ğŸ“Š è§£æå®Œæˆ: {data['total_charging_stations']} ä¸ªå……ç”µæ¡©")
        logging.info(f"ğŸ“Š ä½¿ç”¨è¿‡çš„å……ç”µæ¡©: {data['used_charging_stations']}")
        logging.info(f"ğŸ“Š å‚ä¸å……ç”µçš„EVæ•°: {len(data['ev_charging_participation'])}")
        
    except Exception as e:
        logging.error(f"âŒ è§£æchargingeventsæ•°æ®å¤±è´¥: {e}")
    
    return data

def decompress_file(gz_file_path, xml_file_path):
    """è§£å‹ .xml.gz æ–‡ä»¶åˆ° .xml æ–‡ä»¶"""
    try:
        with gzip.open(gz_file_path, 'rb') as f_in:
            with open(xml_file_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        logging.info(f"âœ… è§£å‹å®Œæˆ: {gz_file_path} -> {xml_file_path}")
        return True
    except Exception as e:
        logging.error(f"âŒ è§£å‹å¤±è´¥ {gz_file_path}: {e}")
        return False

def analyze_scenario_metrics_fixed(scenario_id, output_dir):
    """åˆ†æåœºæ™¯çš„8ä¸ªå…³é”®æŒ‡æ ‡ - ä¿®å¤ç²¾åº¦é—®é¢˜ç‰ˆæœ¬"""
    logging.info(f"ğŸ¯ å¼€å§‹åˆ†æåœºæ™¯: {scenario_id}")
    
    # å®šä¹‰æ–‡ä»¶è·¯å¾„
    gz_files = {
        'tripinfo': os.path.join(output_dir, "tripinfo_output.xml.gz"),
        'charging': os.path.join(output_dir, "chargingevents.xml.gz")
    }
    
    xml_files = {
        'tripinfo': os.path.join(output_dir, "tripinfo_output_temp.xml"),
        'charging': os.path.join(output_dir, "chargingevents_temp.xml")
    }
    
    # æ£€æŸ¥å‹ç¼©æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    missing_files = []
    for name, gz_path in gz_files.items():
        if not os.path.exists(gz_path):
            missing_files.append(gz_path)
    
    if missing_files:
        logging.error(f"âŒ ç¼ºå°‘å‹ç¼©æ–‡ä»¶:")
        for file in missing_files:
            logging.error(f"   - {file}")
        return None
    
    # è§£å‹æ–‡ä»¶
    logging.info("ğŸ”§ å¼€å§‹è§£å‹æ–‡ä»¶...")
    for name, gz_path in gz_files.items():
        xml_path = xml_files[name]
        if not decompress_file(gz_path, xml_path):
            logging.error(f"âŒ è§£å‹å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆ†æ")
            return None
    
    # è§£ææ•°æ®
    tripinfo_data = parse_tripinfo_data_fixed(xml_files['tripinfo'])
    charging_data = parse_charging_events_data_fixed(xml_files['charging'])
    
    # è®¡ç®—8ä¸ªå…³é”®æŒ‡æ ‡ - ä½¿ç”¨ä¿®å¤çš„å‡½æ•°
    metrics = {}
    
    # 1. å¹³å‡durationï¼ˆè½¦è¾†å£å¾„ï¼‰
    duration_stats = calculate_statistics_fixed(tripinfo_data['durations'], "è½¦è¾†è¡Œé©¶æ—¶é—´")
    metrics.update({
        'duration_mean': duration_stats['mean'],
        'duration_median': duration_stats['median'],
        'duration_p90': duration_stats['p90']
    })
    
    # 2. å¹³å‡å……ç”µæ—¶é—´ï¼ˆäº‹ä»¶å£å¾„ï¼‰
    charging_time_stats = calculate_statistics_fixed(charging_data['charging_steps'], "å……ç”µæ—¶é—´")
    metrics.update({
        'charging_time_mean': charging_time_stats['mean'],
        'charging_time_median': charging_time_stats['median'],
        'charging_time_p90': charging_time_stats['p90']
    })
    
    # 3. å¹³å‡ç­‰å¾…æ—¶é—´
    waiting_time_stats = calculate_statistics_fixed(tripinfo_data['waiting_times'], "ç­‰å¾…æ—¶é—´")
    metrics.update({
        'waiting_time_mean': waiting_time_stats['mean'],
        'waiting_time_median': waiting_time_stats['median'],
        'waiting_time_p90': waiting_time_stats['p90']
    })
    
    # 4. ç«™ç‚¹"å……ç”µé‡"çš„ç¦»æ•£ç¨‹åº¦
    energy_values = [e for e in charging_data['total_energy_charged'] if e > 0]
    if energy_values:
        energy_gini = calculate_gini_coefficient(energy_values)
        energy_cv = calculate_cv(energy_values)
        energy_hhi = calculate_hhi(energy_values)
        energy_p90_p50_ratio = round(np.percentile(energy_values, 90) / np.percentile(energy_values, 50), 4) if len(energy_values) > 0 else 0
        zero_usage_rate = round((charging_data['total_charging_stations'] - len(energy_values)) / charging_data['total_charging_stations'], 6)
    else:
        energy_gini = energy_cv = energy_hhi = energy_p90_p50_ratio = 0.0
        zero_usage_rate = 1.0
    
    metrics.update({
        'energy_gini': energy_gini,
        'energy_cv': energy_cv,
        'energy_hhi': energy_hhi,
        'energy_p90_p50_ratio': energy_p90_p50_ratio,
        'energy_zero_usage_rate': zero_usage_rate
    })
    
    # 5. "å……ç”µæ¡©çš„å……ç”µè½¦è¾†æ•°"çš„ç¦»æ•£ç¨‹åº¦
    vehicle_count_values = [v for v in charging_data['charging_vehicles_count'] if v > 0]
    if vehicle_count_values:
        vehicle_gini = calculate_gini_coefficient(vehicle_count_values)
        vehicle_cv = calculate_cv(vehicle_count_values)
        vehicle_hhi = calculate_hhi(vehicle_count_values)
        vehicle_zero_usage_rate = round((charging_data['total_charging_stations'] - len(vehicle_count_values)) / charging_data['total_charging_stations'], 6)
    else:
        vehicle_gini = vehicle_cv = vehicle_hhi = 0.0
        vehicle_zero_usage_rate = 1.0
    
    metrics.update({
        'vehicle_gini': vehicle_gini,
        'vehicle_cv': vehicle_cv,
        'vehicle_hhi': vehicle_hhi,
        'vehicle_zero_usage_rate': vehicle_zero_usage_rate
    })
    
    # 6. å……ç”µæ¡©ä½¿ç”¨è¦†ç›–ç‡
    coverage_rate = round(charging_data['used_charging_stations'] / charging_data['total_charging_stations'], 6)
    metrics['charging_station_coverage'] = coverage_rate
    
    # 7. rerouteæ•°
    metrics['reroute_count'] = tripinfo_data['reroute_count']
    
    # 8. EVå……ç”µå‚ä¸ç‡
    ev_total = 1800  # å›ºå®šå€¼
    ev_participation_rate = round(len(charging_data['ev_charging_participation']) / ev_total, 6)
    metrics['ev_charging_participation_rate'] = ev_participation_rate
    
    # 9. å……ç”µå¤±è´¥çš„EVæ•°
    metrics['ev_charging_failures'] = tripinfo_data['ev_charging_failures']
    
    # æ·»åŠ åœºæ™¯ä¿¡æ¯
    metrics['scenario_id'] = scenario_id
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    logging.info("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    for xml_path in xml_files.values():
        if os.path.exists(xml_path):
            os.remove(xml_path)
    
    # è¾“å‡ºç²¾åº¦æ£€æŸ¥ä¿¡æ¯
    logging.info("ğŸ” ç²¾åº¦æ£€æŸ¥:")
    logging.info(f"   Duration P90: {metrics['duration_p90']}")
    logging.info(f"   Waiting Time P90: {metrics['waiting_time_p90']}")
    logging.info(f"   Charging Time P90: {metrics['charging_time_p90']}")
    
    logging.info("âœ… åˆ†æå®Œæˆ")
    return metrics

def save_results(metrics, output_file):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
    df = pd.DataFrame([metrics])
    df.to_csv(output_file, index=False)
    logging.info(f"âœ… ç»“æœä¿å­˜åˆ°: {output_file}")

def main():
    setup_logging()
    
    parser = argparse.ArgumentParser(description='åˆ†æSUMOè¾“å‡ºæ–‡ä»¶çš„8ä¸ªå…³é”®æŒ‡æ ‡ - ä¿®å¤ç²¾åº¦é—®é¢˜ç‰ˆæœ¬')
    parser.add_argument('--scenario_id', type=str, required=True,
                       help='åœºæ™¯ID (ä¾‹å¦‚: S001)')
    parser.add_argument('--output_dir', type=str, 
                       default='sumo',
                       help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--result_dir', type=str,
                       help='ç»“æœä¿å­˜ç›®å½• (é»˜è®¤: output_dir/scenario_id/result)')
    
    args = parser.parse_args()
    
    # æ„å»ºè·¯å¾„
    scenario_output_dir = os.path.join(args.output_dir, args.scenario_id, "output")
    
    if args.result_dir:
        result_dir = args.result_dir
    else:
        result_dir = os.path.join(args.output_dir, args.scenario_id, "result")
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(result_dir, exist_ok=True)
    
    # åˆ†ææ•°æ®
    metrics = analyze_scenario_metrics_fixed(args.scenario_id, scenario_output_dir)
    
    if metrics:
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(result_dir, "charging_analysis_fixed.csv")
        save_results(metrics, result_file)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“Š 8ä¸ªå…³é”®æŒ‡æ ‡åˆ†æç»“æœæ‘˜è¦ (ä¿®å¤ç²¾åº¦ç‰ˆæœ¬)")
        print("="*60)
        print(f"åœºæ™¯ID: {metrics['scenario_id']}")
        print(f"\n1. è½¦è¾†è¡Œé©¶æ—¶é—´ (ç§’):")
        print(f"   - å¹³å‡: {metrics['duration_mean']:.4f}")
        print(f"   - ä¸­ä½æ•°: {metrics['duration_median']:.4f}")
        print(f"   - P90: {metrics['duration_p90']:.4f}")
        
        print(f"\n3. ç­‰å¾…æ—¶é—´ (ç§’):")
        print(f"   - å¹³å‡: {metrics['waiting_time_mean']:.4f}")
        print(f"   - ä¸­ä½æ•°: {metrics['waiting_time_median']:.4f}")
        print(f"   - P90: {metrics['waiting_time_p90']:.4f}")
        
        print("="*60)
        print("âœ… ç²¾åº¦é—®é¢˜å·²ä¿®å¤ï¼Œä¸å†å‡ºç°é•¿å°¾æ•°")
    else:
        logging.error("âŒ åˆ†æå¤±è´¥")
        sys.exit(1)

if __name__ == '__main__':
    main()


