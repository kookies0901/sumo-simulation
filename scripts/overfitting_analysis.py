#!/usr/bin/env python3
"""
è¿‡æ‹Ÿåˆé—®é¢˜åˆ†æ - ç®€åŒ–ç‰ˆæœ¬
ä¸“é—¨é’ˆå¯¹å°æ ·æœ¬æ•°æ®çš„å›å½’åˆ†æï¼Œæä¾›æ¸…æ™°çš„è¿‡æ‹Ÿåˆè¯Šæ–­
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score, LeaveOneOut
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

def safe_cross_validation(model, X, y):
    """å®‰å…¨çš„äº¤å‰éªŒè¯ï¼Œå¤„ç†å¯èƒ½çš„æ•°å€¼é—®é¢˜"""
    try:
        cv = LeaveOneOut()
        scores = cross_val_score(model, X, y, cv=cv, scoring='r2')
        # è¿‡æ»¤æ‰æ— æ•ˆå€¼
        valid_scores = scores[~np.isnan(scores)]
        if len(valid_scores) > 0:
            return valid_scores.mean(), valid_scores.std(), len(valid_scores)
        else:
            return 0.0, 0.0, 0
    except:
        return 0.0, 0.0, 0

def analyze_single_relationship(df, x_col, y_col):
    """åˆ†æå•ä¸ªç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„å…³ç³»"""
    
    # è·å–æ•°æ®å¹¶æ¸…ç†
    x = df[x_col].values
    y = df[y_col].values
    
    # ç§»é™¤NaNå€¼
    mask = ~(np.isnan(x) | np.isnan(y))
    x_clean = x[mask]
    y_clean = y[mask]
    
    if len(x_clean) < 5:
        return None
    
    # åŸºæœ¬ç»Ÿè®¡
    sample_size = len(x_clean)
    
    # çš®å°”é€Šç›¸å…³ç³»æ•°
    try:
        pearson_r, pearson_p = stats.pearsonr(x_clean, y_clean)
    except:
        pearson_r, pearson_p = 0.0, 1.0
    
    # å‡†å¤‡æ•°æ®
    X = x_clean.reshape(-1, 1)
    
    # 1. çº¿æ€§å›å½’
    linear_model = LinearRegression()
    linear_model.fit(X, y_clean)
    y_pred_linear = linear_model.predict(X)
    train_r2_linear = r2_score(y_clean, y_pred_linear)
    
    # äº¤å‰éªŒè¯
    cv_r2_linear, cv_std_linear, cv_count = safe_cross_validation(linear_model, X, y_clean)
    overfitting_linear = train_r2_linear - cv_r2_linear
    
    # 2. Ridgeå›å½’ï¼ˆæ­£åˆ™åŒ–ï¼‰
    ridge_model = Ridge(alpha=1.0)
    ridge_model.fit(X, y_clean)
    y_pred_ridge = ridge_model.predict(X)
    train_r2_ridge = r2_score(y_clean, y_pred_ridge)
    
    cv_r2_ridge, cv_std_ridge, _ = safe_cross_validation(ridge_model, X, y_clean)
    overfitting_ridge = train_r2_ridge - cv_r2_ridge
    
    # æ•°æ®å˜å¼‚æ€§åˆ†æ
    x_cv = np.std(x_clean) / np.mean(x_clean) if np.mean(x_clean) != 0 else 0
    y_cv = np.std(y_clean) / np.mean(y_clean) if np.mean(y_clean) != 0 else 0
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    if cv_r2_ridge > cv_r2_linear:
        best_model = "Ridge"
        best_cv_r2 = cv_r2_ridge
        best_overfitting = overfitting_ridge
    else:
        best_model = "Linear"
        best_cv_r2 = cv_r2_linear
        best_overfitting = overfitting_linear
    
    # å…³ç³»è´¨é‡è¯„ä¼°
    if abs(pearson_r) < 0.15:
        quality = "ç›¸å…³æ€§æå¼±"
        recommendation = "è€ƒè™‘èˆå¼ƒ"
    elif best_overfitting > 0.5:
        quality = "ä¸¥é‡è¿‡æ‹Ÿåˆ"
        recommendation = "æ•°æ®ä¸è¶³ï¼Œè°¨æ…ä½¿ç”¨"
    elif best_overfitting > 0.3:
        quality = "ä¸­åº¦è¿‡æ‹Ÿåˆ"
        recommendation = "éœ€è¦æ›´å¤šæ•°æ®"
    elif best_cv_r2 < 0.1 and abs(pearson_r) > 0.2:
        quality = "æ¨¡å‹ä¸å½“"
        recommendation = "å°è¯•éçº¿æ€§æ¨¡å‹"
    elif best_cv_r2 > 0.2 and best_overfitting < 0.2:
        quality = "è‰¯å¥½"
        recommendation = "å¯ç”¨äºåˆ†æ"
    else:
        quality = "ä¸€èˆ¬"
        recommendation = "å¯å‚è€ƒä½¿ç”¨"
    
    return {
        'feature': x_col,
        'target': y_col,
        'sample_size': sample_size,
        'pearson_r': pearson_r,
        'pearson_p': pearson_p,
        'linear_train_r2': train_r2_linear,
        'linear_cv_r2': cv_r2_linear,
        'linear_overfitting': overfitting_linear,
        'ridge_train_r2': train_r2_ridge,
        'ridge_cv_r2': cv_r2_ridge,
        'ridge_overfitting': overfitting_ridge,
        'best_model': best_model,
        'best_cv_r2': best_cv_r2,
        'best_overfitting': best_overfitting,
        'x_variability': x_cv,
        'y_variability': y_cv,
        'quality': quality,
        'recommendation': recommendation
    }

def main():
    print("ğŸ” è¿‡æ‹Ÿåˆé—®é¢˜åˆ†æ")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    data_file = "/home/ubuntu/project/MSC/Msc_Project/models/input/merged_dataset.csv"
    
    try:
        df = pd.read_csv(data_file)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # å®šä¹‰å˜é‡
    layout_features = [
        'avg_dist_to_center', 'std_nearest_neighbor', 'min_distance',
        'max_pairwise_distance', 'cs_density_std', 'cluster_count',
        'coverage_ratio', 'max_gap_distance', 'gini_coefficient',
        'avg_betweenness_centrality'
    ]
    
    performance_metrics = [
        'duration_mean', 'waiting_time_mean', 'charging_time_mean',
        'energy_gini', 'vehicle_gini', 'charging_station_coverage',
        'reroute_count', 'ev_charging_participation_rate'
    ]
    
    print(f"ğŸ“Š åˆ†æ {len(layout_features)} ä¸ªå¸ƒå±€ç‰¹å¾ vs {len(performance_metrics)} ä¸ªæ€§èƒ½æŒ‡æ ‡")
    print(f"ğŸ”¢ æ€»å…± {len(layout_features) * len(performance_metrics)} ä¸ªå…³ç³»")
    
    results = []
    
    # é€ä¸€åˆ†æ
    for i, feature in enumerate(layout_features):
        print(f"\n[{i+1}/{len(layout_features)}] åˆ†æç‰¹å¾: {feature}")
        
        for target in performance_metrics:
            if feature in df.columns and target in df.columns:
                result = analyze_single_relationship(df, feature, target)
                if result:
                    results.append(result)
                    print(f"  {target}: r={result['pearson_r']:.3f}, "
                          f"CV RÂ²={result['best_cv_r2']:.3f}, "
                          f"è¿‡æ‹Ÿåˆ={result['best_overfitting']:.3f}, "
                          f"{result['quality']}")
    
    # ä¿å­˜å’Œåˆ†æç»“æœ
    if results:
        results_df = pd.DataFrame(results)
        
        print(f"\n" + "=" * 60)
        print(f"ğŸ“Š è¿‡æ‹Ÿåˆåˆ†ææ±‡æ€»æŠ¥å‘Š")
        print(f"=" * 60)
        
        print(f"âœ… æˆåŠŸåˆ†æäº† {len(results)} ä¸ªå…³ç³»")
        
        # è´¨é‡åˆ†å¸ƒ
        print(f"\nğŸ¯ å…³ç³»è´¨é‡åˆ†å¸ƒ:")
        quality_counts = results_df['quality'].value_counts()
        for quality, count in quality_counts.items():
            percentage = count / len(results) * 100
            print(f"   {quality}: {count} ä¸ª ({percentage:.1f}%)")
        
        # è¿‡æ‹Ÿåˆä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        print(f"\nâš ï¸ è¿‡æ‹Ÿåˆä¸¥é‡ç¨‹åº¦:")
        overfitting_severe = len(results_df[results_df['best_overfitting'] > 0.5])
        overfitting_moderate = len(results_df[(results_df['best_overfitting'] > 0.3) & (results_df['best_overfitting'] <= 0.5)])
        overfitting_mild = len(results_df[(results_df['best_overfitting'] > 0.1) & (results_df['best_overfitting'] <= 0.3)])
        overfitting_none = len(results_df[results_df['best_overfitting'] <= 0.1])
        
        print(f"   ä¸¥é‡è¿‡æ‹Ÿåˆ (>0.5): {overfitting_severe} ä¸ª")
        print(f"   ä¸­åº¦è¿‡æ‹Ÿåˆ (0.3-0.5): {overfitting_moderate} ä¸ª") 
        print(f"   è½»åº¦è¿‡æ‹Ÿåˆ (0.1-0.3): {overfitting_mild} ä¸ª")
        print(f"   æ— è¿‡æ‹Ÿåˆ (â‰¤0.1): {overfitting_none} ä¸ª")
        
        # æœ€ä½³å…³ç³»ï¼ˆå¯ç”¨äºè®ºæ–‡ï¼‰
        print(f"\nğŸ† æ¨èç”¨äºè®ºæ–‡çš„å…³ç³» (è´¨é‡='è‰¯å¥½'):")
        good_relationships = results_df[results_df['quality'] == 'è‰¯å¥½'].sort_values('best_cv_r2', ascending=False)
        
        if len(good_relationships) > 0:
            for _, row in good_relationships.iterrows():
                print(f"   âœ… {row['feature']} -> {row['target']}")
                print(f"      ç›¸å…³ç³»æ•°: {row['pearson_r']:.4f}, CV RÂ²: {row['best_cv_r2']:.4f}, è¿‡æ‹Ÿåˆ: {row['best_overfitting']:.4f}")
        else:
            print(f"   âŒ æ²¡æœ‰å‘ç°è´¨é‡ä¸º'è‰¯å¥½'çš„å…³ç³»")
        
        # éœ€è¦è°¨æ…å¤„ç†çš„å…³ç³»
        print(f"\nâš ï¸ éœ€è¦è°¨æ…å¤„ç†çš„å…³ç³»:")
        problematic = results_df[results_df['quality'].isin(['ä¸¥é‡è¿‡æ‹Ÿåˆ', 'ä¸­åº¦è¿‡æ‹Ÿåˆ'])]
        if len(problematic) > 0:
            for _, row in problematic.head(5).iterrows():
                print(f"   âš ï¸ {row['feature']} -> {row['target']}: {row['quality']}")
                print(f"      è¿‡æ‹Ÿåˆç¨‹åº¦: {row['best_overfitting']:.4f}, å»ºè®®: {row['recommendation']}")
        
        # æ•°æ®é—®é¢˜è¯Šæ–­
        print(f"\nğŸ” æ•°æ®é—®é¢˜è¯Šæ–­:")
        avg_overfitting = results_df['best_overfitting'].mean()
        avg_cv_r2 = results_df['best_cv_r2'].mean()
        
        print(f"   å¹³å‡è¿‡æ‹Ÿåˆç¨‹åº¦: {avg_overfitting:.4f}")
        print(f"   å¹³å‡äº¤å‰éªŒè¯RÂ²: {avg_cv_r2:.4f}")
        print(f"   æ ·æœ¬é‡: {results_df['sample_size'].iloc[0]}")
        
        if avg_overfitting > 0.3:
            print(f"   ğŸš¨ æ•´ä½“è¿‡æ‹Ÿåˆä¸¥é‡ï¼Œä¸»è¦åŸå› :")
            print(f"      1. æ ·æœ¬é‡å¤ªå° (N={len(df)})")
            print(f"      2. ç‰¹å¾ä¸ç›®æ ‡å˜é‡çœŸå®ç›¸å…³æ€§å¯èƒ½è¾ƒå¼±")
            print(f"      3. éœ€è¦æ”¶é›†æ›´å¤šæ•°æ®")
        
        # ä¿å­˜ç»“æœ
        output_file = "/home/ubuntu/project/MSC/Msc_Project/models/analysis_simple/overfitting_analysis.csv"
        results_df.to_csv(output_file, index=False)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ€»ç»“å»ºè®®
        print(f"\nğŸ“ æ€»ç»“å»ºè®®:")
        good_count = len(good_relationships)
        if good_count > 0:
            print(f"   âœ… å‘ç° {good_count} ä¸ªå¯ç”¨å…³ç³»ï¼Œå»ºè®®åœ¨è®ºæ–‡ä¸­é‡ç‚¹å±•ç¤º")
            print(f"   ğŸ“Š ä½¿ç”¨ç®€å•çº¿æ€§å›å½’æˆ–Ridgeå›å½’")
            print(f"   ğŸ“ˆ æŠ¥å‘Šäº¤å‰éªŒè¯RÂ²è€Œéè®­ç»ƒRÂ²")
        else:
            print(f"   âš ï¸ å½“å‰æ•°æ®éš¾ä»¥å»ºç«‹ç¨³å®šçš„å›å½’å…³ç³»")
            print(f"   ğŸ’¡ å»ºè®®:")
            print(f"      - å¢åŠ æ ·æœ¬é‡ï¼ˆç›®æ ‡>100ä¸ªå¸ƒå±€ï¼‰")
            print(f"      - è€ƒè™‘æè¿°æ€§ç»Ÿè®¡åˆ†æè€Œéå›å½’")
            print(f"      - ä½¿ç”¨ç›¸å…³æ€§åˆ†æä»£æ›¿å›å½’åˆ†æ")
    
    return 0

if __name__ == '__main__':
    exit(main())
