#!/usr/bin/env python3
"""
åˆ†æSUMOè¾“å‡ºæ–‡ä»¶ - é‡æ„ç‰ˆæœ¬
åªè§£å‹ tripinfo_output.xml.gz å’Œ chargingevents.xml.gzï¼Œåˆ†æ8ä¸ªå…³é”®æŒ‡æ ‡
æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªåœºæ™¯
"""

import os
import sys
import csv
import json
import argparse
import xml.etree.ElementTree as ET
import pandas as pd
import logging
import gzip
import shutil
import numpy as np
from datetime import datetime
from vehicle_config import load_vehicle_config

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )

def load_scenario_matrix(matrix_file):
    """åŠ è½½åœºæ™¯çŸ©é˜µæ–‡ä»¶"""
    try:
        scenarios = []
        with open(matrix_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                scenarios.append(row)
        logging.info(f"ğŸ“Š ä»åœºæ™¯çŸ©é˜µåŠ è½½äº† {len(scenarios)} ä¸ªåœºæ™¯")
        return scenarios
    except Exception as e:
        logging.error(f"âŒ åŠ è½½åœºæ™¯çŸ©é˜µå¤±è´¥: {e}")
        return []

def get_scenario_list(start_id="S001", end_id="S050"):
    """ç”Ÿæˆåœºæ™¯IDåˆ—è¡¨ï¼Œæ”¯æŒS001-S050æ ¼å¼"""
    try:
        # æå–æ•°å­—éƒ¨åˆ†
        start_num = int(start_id[1:])
        end_num = int(end_id[1:])
        
        scenarios = []
        for i in range(start_num, end_num + 1):
            scenario_id = f"S{i:03d}"
            scenarios.append(scenario_id)
        
        logging.info(f"ğŸ“Š ç”Ÿæˆäº† {len(scenarios)} ä¸ªåœºæ™¯ID: {start_id} åˆ° {end_id}")
        return scenarios
    except Exception as e:
        logging.error(f"âŒ ç”Ÿæˆåœºæ™¯åˆ—è¡¨å¤±è´¥: {e}")
        return []

def decompress_file(gz_file_path, xml_file_path):
    """è§£å‹ .xml.gz æ–‡ä»¶åˆ° .xml æ–‡ä»¶"""
    try:
        with gzip.open(gz_file_path, 'rb') as f_in:
            with open(xml_file_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        logging.info(f"âœ… è§£å‹å®Œæˆ: {gz_file_path} -> {xml_file_path}")
        return True
    except Exception as e:
        logging.error(f"âŒ è§£å‹å¤±è´¥ {gz_file_path}: {e}")
        return False

def calculate_statistics(values, name="æ•°æ®"):
    """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ï¼šmean, median, p90"""
    if not values:
        return {"mean": 0.0, "median": 0.0, "p90": 0.0}
    
    values = np.array(values)
    mean = np.mean(values)
    median = np.median(values)
    p90 = np.percentile(values, 90)
    
    logging.info(f"ğŸ“Š {name}ç»Ÿè®¡: mean={mean:.2f}, median={median:.2f}, p90={p90:.2f}")
    return {"mean": mean, "median": median, "p90": p90}

def calculate_gini_coefficient(values):
    """è®¡ç®—åŸºå°¼ç³»æ•°"""
    if not values or len(values) == 0:
        return 0.0
    
    values = np.array(values)
    if np.sum(values) == 0:
        return 0.0
    
    # æ’åºå¹¶è®¡ç®—ç´¯ç§¯ä»½é¢
    sorted_values = np.sort(values)
    n = len(sorted_values)
    cumsum = np.cumsum(sorted_values)
    
    # è®¡ç®—åŸºå°¼ç³»æ•°
    gini = (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n
    return gini

def calculate_hhi(values):
    """è®¡ç®—HHIæŒ‡æ•° (Herfindahl-Hirschman Index)"""
    if not values or len(values) == 0:
        return 0.0
    
    values = np.array(values)
    total = np.sum(values)
    if total == 0:
        return 0.0
    
    # è®¡ç®—å¸‚åœºä»½é¢çš„å¹³æ–¹å’Œ
    market_shares = values / total
    hhi = np.sum(market_shares ** 2)
    return hhi

def calculate_cv(values):
    """è®¡ç®—å˜å¼‚ç³»æ•° (Coefficient of Variation)"""
    if not values or len(values) == 0:
        return 0.0
    
    values = np.array(values)
    mean = np.mean(values)
    if mean == 0:
        return 0.0
    
    std = np.std(values)
    cv = std / mean
    return cv

def parse_tripinfo_data(xml_file_path):
    """è§£ætripinfoæ•°æ®ï¼Œè·å–è½¦è¾†durationã€waitingTimeã€rerouteNoç­‰ä¿¡æ¯"""
    logging.info(f"ğŸ” å¼€å§‹è§£ætripinfoæ•°æ®: {xml_file_path}")
    
    data = {
        'durations': [],
        'waiting_times': [],
        'reroute_count': 0,
        'ev_charging_failures': 0,
        'total_vehicles': 0
    }
    
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()
        
        for tripinfo in root.findall("tripinfo"):
            data['total_vehicles'] += 1
            
            # è·å–duration
            duration = float(tripinfo.get("duration", 0))
            data['durations'].append(duration)
            
            # è·å–waitingTime
            waiting_time = float(tripinfo.get("waitingTime", 0))
            data['waiting_times'].append(waiting_time)
            
            # ç»Ÿè®¡rerouteæ¬¡æ•°
            reroute_no = int(tripinfo.get("rerouteNo", 0))
            if reroute_no > 0:
                data['reroute_count'] += 1
        
        # ç»Ÿè®¡stationfinderå…ƒç´ æ•°é‡ï¼ˆå……ç”µå¤±è´¥çš„EVï¼‰
        # stationfinderæ˜¯tripinfoçš„å­å…ƒç´ ï¼Œéœ€è¦é€’å½’æŸ¥æ‰¾
        stationfinder_count = 0
        for tripinfo in root.findall("tripinfo"):
            stationfinders = tripinfo.findall("stationfinder")
            stationfinder_count += len(stationfinders)
        data['ev_charging_failures'] = stationfinder_count
        
        logging.info(f"ğŸ“Š è§£æå®Œæˆ: {data['total_vehicles']} è¾†è½¦")
        logging.info(f"ğŸ“Š é‡æ–°è·¯ç”±è½¦è¾†æ•°: {data['reroute_count']}")
        logging.info(f"ğŸ“Š EVå……ç”µå¤±è´¥æ•°: {data['ev_charging_failures']}")
        
    except Exception as e:
        logging.error(f"âŒ è§£ætripinfoæ•°æ®å¤±è´¥: {e}")
    
    return data

def parse_charging_events_data(xml_file_path):
    """è§£æchargingeventsæ•°æ®ï¼Œè·å–å……ç”µæ¡©ä½¿ç”¨æƒ…å†µ"""
    logging.info(f"ğŸ” å¼€å§‹è§£æchargingeventsæ•°æ®: {xml_file_path}")
    
    data = {
        'charging_steps': [],
        'total_energy_charged': [],
        'charging_vehicles_count': [],
        'ev_charging_participation': set(),
        'total_charging_stations': 0,
        'used_charging_stations': 0
    }
    
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()
        
        for station in root.findall("chargingStation"):
            data['total_charging_stations'] += 1
            
            # è·å–å……ç”µæ—¶é—´
            charging_steps = int(station.get("chargingSteps", 0))
            if charging_steps > 0:
                data['charging_steps'].append(charging_steps)
            
            # è·å–æ€»å……ç”µé‡
            total_energy = float(station.get("totalEnergyCharged", 0))
            data['total_energy_charged'].append(total_energy)
            
            if total_energy > 0:
                data['used_charging_stations'] += 1
            
            # ç»Ÿè®¡å……ç”µè½¦è¾†æ•°
            vehicle_count = len(station.findall("vehicle"))
            data['charging_vehicles_count'].append(vehicle_count)
            
            # ç»Ÿè®¡å‚ä¸å……ç”µçš„EV
            for vehicle in station.findall("vehicle"):
                veh_id = vehicle.get("id", "")
                if veh_id.startswith("EV_"):
                    data['ev_charging_participation'].add(veh_id)
        
        logging.info(f"ğŸ“Š è§£æå®Œæˆ: {data['total_charging_stations']} ä¸ªå……ç”µæ¡©")
        logging.info(f"ğŸ“Š ä½¿ç”¨è¿‡çš„å……ç”µæ¡©: {data['used_charging_stations']}")
        logging.info(f"ğŸ“Š å‚ä¸å……ç”µçš„EVæ•°: {len(data['ev_charging_participation'])}")
        
    except Exception as e:
        logging.error(f"âŒ è§£æchargingeventsæ•°æ®å¤±è´¥: {e}")
    
    return data

def analyze_scenario_metrics(scenario_id, output_dir):
    """åˆ†æåœºæ™¯çš„8ä¸ªå…³é”®æŒ‡æ ‡"""
    logging.info(f"ğŸ¯ å¼€å§‹åˆ†æåœºæ™¯: {scenario_id}")
    
    # å®šä¹‰æ–‡ä»¶è·¯å¾„
    gz_files = {
        'tripinfo': os.path.join(output_dir, "tripinfo_output.xml.gz"),
        'charging': os.path.join(output_dir, "chargingevents.xml.gz")
    }
    
    xml_files = {
        'tripinfo': os.path.join(output_dir, "tripinfo_output_temp.xml"),
        'charging': os.path.join(output_dir, "chargingevents_temp.xml")
    }
    
    # æ£€æŸ¥å‹ç¼©æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    missing_files = []
    for name, gz_path in gz_files.items():
        if not os.path.exists(gz_path):
            missing_files.append(gz_path)
    
    if missing_files:
        logging.error(f"âŒ ç¼ºå°‘å‹ç¼©æ–‡ä»¶:")
        for file in missing_files:
            logging.error(f"   - {file}")
        return None
    
    # è§£å‹æ–‡ä»¶
    logging.info("ğŸ”§ å¼€å§‹è§£å‹æ–‡ä»¶...")
    for name, gz_path in gz_files.items():
        xml_path = xml_files[name]
        if not decompress_file(gz_path, xml_path):
            logging.error(f"âŒ è§£å‹å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆ†æ")
            return None
    
    # è§£ææ•°æ®
    tripinfo_data = parse_tripinfo_data(xml_files['tripinfo'])
    charging_data = parse_charging_events_data(xml_files['charging'])
    
    # è®¡ç®—8ä¸ªå…³é”®æŒ‡æ ‡
    metrics = {}
    
    # 1. å¹³å‡durationï¼ˆè½¦è¾†å£å¾„ï¼‰
    duration_stats = calculate_statistics(tripinfo_data['durations'], "è½¦è¾†è¡Œé©¶æ—¶é—´")
    metrics.update({
        'duration_mean': duration_stats['mean'],
        'duration_median': duration_stats['median'],
        'duration_p90': duration_stats['p90']
    })
    
    # 2. å¹³å‡å……ç”µæ—¶é—´ï¼ˆäº‹ä»¶å£å¾„ï¼‰
    charging_time_stats = calculate_statistics(charging_data['charging_steps'], "å……ç”µæ—¶é—´")
    metrics.update({
        'charging_time_mean': charging_time_stats['mean'],
        'charging_time_median': charging_time_stats['median'],
        'charging_time_p90': charging_time_stats['p90']
    })
    
    # 3. å¹³å‡ç­‰å¾…æ—¶é—´
    waiting_time_stats = calculate_statistics(tripinfo_data['waiting_times'], "ç­‰å¾…æ—¶é—´")
    metrics.update({
        'waiting_time_mean': waiting_time_stats['mean'],
        'waiting_time_median': waiting_time_stats['median'],
        'waiting_time_p90': waiting_time_stats['p90']
    })
    
    # 4. ç«™ç‚¹"å……ç”µé‡"çš„ç¦»æ•£ç¨‹åº¦
    energy_values = [e for e in charging_data['total_energy_charged'] if e > 0]
    if energy_values:
        energy_gini = calculate_gini_coefficient(energy_values)
        energy_cv = calculate_cv(energy_values)
        energy_hhi = calculate_hhi(energy_values)
        energy_p90_p50_ratio = np.percentile(energy_values, 90) / np.percentile(energy_values, 50) if len(energy_values) > 0 else 0
        zero_usage_rate = (charging_data['total_charging_stations'] - len(energy_values)) / charging_data['total_charging_stations']
    else:
        energy_gini = energy_cv = energy_hhi = energy_p90_p50_ratio = 0.0
        zero_usage_rate = 1.0
    
    metrics.update({
        'energy_gini': energy_gini,
        'energy_cv': energy_cv,
        'energy_hhi': energy_hhi,
        'energy_p90_p50_ratio': energy_p90_p50_ratio,
        'energy_zero_usage_rate': zero_usage_rate
    })
    
    # 5. "å……ç”µæ¡©çš„å……ç”µè½¦è¾†æ•°"çš„ç¦»æ•£ç¨‹åº¦
    vehicle_count_values = [v for v in charging_data['charging_vehicles_count'] if v > 0]
    if vehicle_count_values:
        vehicle_gini = calculate_gini_coefficient(vehicle_count_values)
        vehicle_cv = calculate_cv(vehicle_count_values)
        vehicle_hhi = calculate_hhi(vehicle_count_values)
        vehicle_zero_usage_rate = (charging_data['total_charging_stations'] - len(vehicle_count_values)) / charging_data['total_charging_stations']
    else:
        vehicle_gini = vehicle_cv = vehicle_hhi = 0.0
        vehicle_zero_usage_rate = 1.0
    
    metrics.update({
        'vehicle_gini': vehicle_gini,
        'vehicle_cv': vehicle_cv,
        'vehicle_hhi': vehicle_hhi,
        'vehicle_zero_usage_rate': vehicle_zero_usage_rate
    })
    
    # 6. å……ç”µæ¡©ä½¿ç”¨è¦†ç›–ç‡
    coverage_rate = charging_data['used_charging_stations'] / charging_data['total_charging_stations']
    metrics['charging_station_coverage'] = coverage_rate
    
    # 7. rerouteæ•°
    metrics['reroute_count'] = tripinfo_data['reroute_count']
    
    # 8. EVå……ç”µå‚ä¸ç‡
    ev_total = 1800  # å›ºå®šå€¼
    ev_participation_rate = len(charging_data['ev_charging_participation']) / ev_total
    metrics['ev_charging_participation_rate'] = ev_participation_rate
    
    # 9. å……ç”µå¤±è´¥çš„EVæ•°
    metrics['ev_charging_failures'] = tripinfo_data['ev_charging_failures']
    
    # æ·»åŠ åœºæ™¯ä¿¡æ¯
    metrics['scenario_id'] = scenario_id
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    logging.info("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    for xml_path in xml_files.values():
        if os.path.exists(xml_path):
            os.remove(xml_path)
    
    logging.info("âœ… åˆ†æå®Œæˆ")
    return metrics

def save_results(metrics, output_file):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
    df = pd.DataFrame([metrics])
    df.to_csv(output_file, index=False)
    logging.info(f"âœ… ç»“æœä¿å­˜åˆ°: {output_file}")

def check_scenario_files(scenario_id, output_dir):
    """æ£€æŸ¥åœºæ™¯æ‰€éœ€çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    scenario_output_dir = os.path.join(output_dir, scenario_id, "output")
    
    required_files = [
        os.path.join(scenario_output_dir, "tripinfo_output.xml.gz"),
        os.path.join(scenario_output_dir, "chargingevents.xml.gz")
    ]
    
    missing_files = []
    for file_path in required_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    return len(missing_files) == 0, missing_files

def batch_analyze_scenarios(scenario_ids, output_dir, result_dir=None):
    """æ‰¹é‡åˆ†æå¤šä¸ªåœºæ™¯"""
    logging.info(f"ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ {len(scenario_ids)} ä¸ªåœºæ™¯")
    
    # é¦–å…ˆæ£€æŸ¥æ‰€æœ‰åœºæ™¯çš„æ–‡ä»¶å¯ç”¨æ€§
    logging.info("ğŸ” æ£€æŸ¥åœºæ™¯æ–‡ä»¶å¯ç”¨æ€§...")
    available_scenarios = []
    missing_scenarios = []
    
    for scenario_id in scenario_ids:
        files_exist, missing_files = check_scenario_files(scenario_id, output_dir)
        if files_exist:
            available_scenarios.append(scenario_id)
        else:
            missing_scenarios.append((scenario_id, missing_files))
            logging.warning(f"âš ï¸ åœºæ™¯ {scenario_id} ç¼ºå°‘æ–‡ä»¶:")
            for missing_file in missing_files:
                logging.warning(f"   - {missing_file}")
    
    logging.info(f"ğŸ“Š æ–‡ä»¶æ£€æŸ¥ç»“æœ: {len(available_scenarios)}/{len(scenario_ids)} ä¸ªåœºæ™¯å¯ç”¨")
    
    if not available_scenarios:
        logging.error("âŒ æ²¡æœ‰å¯ç”¨çš„åœºæ™¯è¿›è¡Œåˆ†æ")
        return []
    
    all_results = []
    success_count = 0
    failure_count = 0
    
    for i, scenario_id in enumerate(available_scenarios, 1):
        logging.info(f"\n[{i}/{len(available_scenarios)}] å¤„ç†åœºæ™¯: {scenario_id}")
        
        try:
            # æ„å»ºå•ä¸ªåœºæ™¯çš„è¾“å‡ºè·¯å¾„
            scenario_output_dir = os.path.join(output_dir, scenario_id, "output")
            
            # åˆ†æå•ä¸ªåœºæ™¯
            metrics = analyze_scenario_metrics(scenario_id, scenario_output_dir)
            
            if metrics:
                all_results.append(metrics)
                success_count += 1
                logging.info(f"âœ… åœºæ™¯ {scenario_id} åˆ†æå®Œæˆ")
            else:
                failure_count += 1
                logging.error(f"âŒ åœºæ™¯ {scenario_id} åˆ†æå¤±è´¥")
                
        except Exception as e:
            failure_count += 1
            logging.error(f"âŒ åœºæ™¯ {scenario_id} å¤„ç†å¼‚å¸¸: {e}")
            continue
    
    # ä¿å­˜æ‰¹é‡ç»“æœ
    if all_results:
        if result_dir is None:
            result_dir = os.path.join(output_dir, "batch_results")
        
        os.makedirs(result_dir, exist_ok=True)
        
        # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°CSV
        batch_result_file = os.path.join(result_dir, "batch_charging_analysis.csv")
        df = pd.DataFrame(all_results)
        df.to_csv(batch_result_file, index=False)
        
        # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
        summary_file = os.path.join(result_dir, "batch_summary.csv")
        summary_data = {
            'total_scenarios_requested': len(scenario_ids),
            'scenarios_with_files': len(available_scenarios),
            'scenarios_missing_files': len(missing_scenarios),
            'successful_analyses': success_count,
            'failed_analyses': failure_count,
            'success_rate_of_available': success_count / len(available_scenarios) if available_scenarios else 0,
            'overall_success_rate': success_count / len(scenario_ids),
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        summary_df = pd.DataFrame([summary_data])
        summary_df.to_csv(summary_file, index=False)
        
        logging.info(f"\nğŸ‰ æ‰¹é‡åˆ†æå®Œæˆï¼")
        logging.info(f"ğŸ“Š è¯·æ±‚å¤„ç†åœºæ™¯æ•°: {len(scenario_ids)}")
        logging.info(f"ğŸ“ æœ‰æ–‡ä»¶çš„åœºæ™¯æ•°: {len(available_scenarios)}")
        logging.info(f"ğŸ“‚ ç¼ºå°‘æ–‡ä»¶çš„åœºæ™¯æ•°: {len(missing_scenarios)}")
        logging.info(f"âœ… åˆ†ææˆåŠŸ: {success_count}")
        logging.info(f"âŒ åˆ†æå¤±è´¥: {failure_count}")
        logging.info(f"ğŸ“ˆ å¯ç”¨åœºæ™¯æˆåŠŸç‡: {success_count/len(available_scenarios)*100:.1f}%" if available_scenarios else "ğŸ“ˆ å¯ç”¨åœºæ™¯æˆåŠŸç‡: 0%")
        logging.info(f"ğŸ“Š æ€»ä½“æˆåŠŸç‡: {success_count/len(scenario_ids)*100:.1f}%")
        logging.info(f"ğŸ’¾ ç»“æœä¿å­˜åˆ°: {batch_result_file}")
        logging.info(f"ğŸ“‹ æ±‡æ€»ä¿å­˜åˆ°: {summary_file}")
        
        return all_results
    else:
        logging.error("âŒ æ²¡æœ‰æˆåŠŸåˆ†æçš„åœºæ™¯")
        return []

def main():
    setup_logging()
    
    parser = argparse.ArgumentParser(description='åˆ†æSUMOè¾“å‡ºæ–‡ä»¶çš„8ä¸ªå…³é”®æŒ‡æ ‡ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†')
    parser.add_argument('--scenario_id', type=str,
                       help='å•ä¸ªåœºæ™¯ID (ä¾‹å¦‚: S001)')
    parser.add_argument('--output_dir', type=str, 
                       default='sumo',
                       help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--result_dir', type=str,
                       help='ç»“æœä¿å­˜ç›®å½• (é»˜è®¤: output_dir/scenario_id/result)')
    
    # æ‰¹é‡å¤„ç†å‚æ•°
    parser.add_argument('--batch', action='store_true',
                       help='å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼')
    parser.add_argument('--start_id', type=str, default='S001',
                       help='æ‰¹é‡å¤„ç†èµ·å§‹åœºæ™¯ID (é»˜è®¤: S001)')
    parser.add_argument('--end_id', type=str, default='S050',
                       help='æ‰¹é‡å¤„ç†ç»“æŸåœºæ™¯ID (é»˜è®¤: S050)')
    parser.add_argument('--matrix', type=str,
                       help='åœºæ™¯çŸ©é˜µæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæ‰¹é‡å¤„ç†')
    parser.add_argument('--all', action='store_true',
                       help='å¤„ç†æ‰€æœ‰å¯ç”¨åœºæ™¯ (è‡ªåŠ¨æ£€æµ‹)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å‚æ•°ç»„åˆ
    if args.batch or args.start_id != 'S001' or args.end_id != 'S050' or args.matrix or args.all:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        if args.matrix:
            # ä»åœºæ™¯çŸ©é˜µæ–‡ä»¶è¯»å–åœºæ™¯åˆ—è¡¨
            scenarios = load_scenario_matrix(args.matrix)
            if scenarios:
                scenario_ids = [s['scenario_id'] for s in scenarios]
                logging.info(f"ğŸ“Š ä»åœºæ™¯çŸ©é˜µè¯»å–åˆ° {len(scenario_ids)} ä¸ªåœºæ™¯")
            else:
                logging.error("âŒ æ— æ³•ä»åœºæ™¯çŸ©é˜µè¯»å–åœºæ™¯åˆ—è¡¨")
                sys.exit(1)
        elif args.all:
            # è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯ç”¨åœºæ™¯
            scenario_ids = []
            for item in os.listdir(args.output_dir):
                if os.path.isdir(os.path.join(args.output_dir, item)) and item.startswith('S') and len(item) == 4:
                    try:
                        int(item[1:])  # éªŒè¯æ ¼å¼
                        scenario_ids.append(item)
                    except ValueError:
                        continue
            
            scenario_ids.sort()
            logging.info(f"ğŸ“Š è‡ªåŠ¨æ£€æµ‹åˆ° {len(scenario_ids)} ä¸ªåœºæ™¯")
        else:
            # ä½¿ç”¨æŒ‡å®šçš„åœºæ™¯èŒƒå›´
            scenario_ids = get_scenario_list(args.start_id, args.end_id)
        
        if not scenario_ids:
            logging.error("âŒ æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„åœºæ™¯")
            sys.exit(1)
        
        # æ‰§è¡Œæ‰¹é‡åˆ†æ
        batch_analyze_scenarios(scenario_ids, args.output_dir, args.result_dir)
        
    elif args.scenario_id:
        # å•ä¸ªåœºæ™¯å¤„ç†æ¨¡å¼
        # æ„å»ºè·¯å¾„
        scenario_output_dir = os.path.join(args.output_dir, args.scenario_id, "output")
        
        if args.result_dir:
            result_dir = args.result_dir
        else:
            result_dir = os.path.join(args.output_dir, args.scenario_id, "result")
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs(result_dir, exist_ok=True)
        
        # åˆ†ææ•°æ®
        metrics = analyze_scenario_metrics(args.scenario_id, scenario_output_dir)
        
        if metrics:
            # ä¿å­˜ç»“æœ
            result_file = os.path.join(result_dir, "charging_analysis.csv")
            save_results(metrics, result_file)
            
            # æ‰“å°ç»“æœæ‘˜è¦
            print("\n" + "="*60)
            print("ğŸ“Š 8ä¸ªå…³é”®æŒ‡æ ‡åˆ†æç»“æœæ‘˜è¦")
            print("="*60)
            print(f"åœºæ™¯ID: {metrics['scenario_id']}")
            print(f"\n1. è½¦è¾†è¡Œé©¶æ—¶é—´ (ç§’):")
            print(f"   - å¹³å‡: {metrics['duration_mean']:.2f}")
            print(f"   - ä¸­ä½æ•°: {metrics['duration_median']:.2f}")
            print(f"   - P90: {metrics['duration_p90']:.2f}")
            
            print(f"\n2. å……ç”µæ—¶é—´ (ç§’):")
            print(f"   - å¹³å‡: {metrics['charging_time_mean']:.2f}")
            print(f"   - ä¸­ä½æ•°: {metrics['charging_time_median']:.2f}")
            print(f"   - P90: {metrics['charging_time_p90']:.2f}")
            
            print(f"\n3. ç­‰å¾…æ—¶é—´ (ç§’):")
            print(f"   - å¹³å‡: {metrics['waiting_time_mean']:.2f}")
            print(f"   - ä¸­ä½æ•°: {metrics['waiting_time_median']:.2f}")
            print(f"   - P90: {metrics['waiting_time_p90']:.2f}")
            
            print(f"\n4. å……ç”µé‡ç¦»æ•£ç¨‹åº¦:")
            print(f"   - åŸºå°¼ç³»æ•°: {metrics['energy_gini']:.4f}")
            print(f"   - å˜å¼‚ç³»æ•°: {metrics['energy_cv']:.4f}")
            print(f"   - HHIæŒ‡æ•°: {metrics['energy_hhi']:.4f}")
            print(f"   - P90/P50æ¯”: {metrics['energy_p90_p50_ratio']:.4f}")
            print(f"   - é›¶ä½¿ç”¨ç‡: {metrics['energy_zero_usage_rate']:.4f}")
            
            print(f"\n5. å……ç”µè½¦è¾†æ•°ç¦»æ•£ç¨‹åº¦:")
            print(f"   - åŸºå°¼ç³»æ•°: {metrics['vehicle_gini']:.4f}")
            print(f"   - å˜å¼‚ç³»æ•°: {metrics['vehicle_cv']:.4f}")
            print(f"   - HHIæŒ‡æ•°: {metrics['vehicle_hhi']:.4f}")
            print(f"   - é›¶ä½¿ç”¨ç‡: {metrics['vehicle_zero_usage_rate']:.4f}")
            
            print(f"\n6. å……ç”µæ¡©ä½¿ç”¨è¦†ç›–ç‡: {metrics['charging_station_coverage']:.4f}")
            print(f"7. é‡æ–°è·¯ç”±è½¦è¾†æ•°: {metrics['reroute_count']}")
            print(f"8. EVå……ç”µå‚ä¸ç‡: {metrics['ev_charging_participation_rate']:.4f}")
            print(f"9. EVå……ç”µå¤±è´¥æ•°: {metrics['ev_charging_failures']}")
            print("="*60)
        else:
            logging.error("âŒ åˆ†æå¤±è´¥")
            sys.exit(1)
    else:
        # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        parser.print_help()
        print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print("   # åˆ†æå•ä¸ªåœºæ™¯")
        print("   python analyze_compressed_output.py --scenario_id S001")
        print("\n   # æ‰¹é‡å¤„ç†S001-S050")
        print("   python analyze_compressed_output.py --batch --start_id S001 --end_id S050")
        print("\n   # ä»åœºæ™¯çŸ©é˜µæ–‡ä»¶æ‰¹é‡å¤„ç†")
        print("   python analyze_compressed_output.py --matrix data/scenario_matrix.csv")
        print("\n   # è‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†æ‰€æœ‰åœºæ™¯")
        print("   python analyze_compressed_output.py --all")

if __name__ == '__main__':
    main() 