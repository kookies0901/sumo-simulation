#!/usr/bin/env python3
"""
åŠ¨æ€æ€§èƒ½æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ
è®¡ç®—81ä¸ªå¸ƒå±€æ–¹æ¡ˆçš„20ä¸ªåŠ¨æ€æ€§èƒ½æŒ‡æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶ç”Ÿæˆä¸“ä¸šçƒ­åŠ›å›¾
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import argparse
from scipy.stats import pearsonr, spearmanr
from scipy.cluster.hierarchy import dendrogram, linkage
from matplotlib.patches import Rectangle
import warnings
warnings.filterwarnings('ignore')

def load_and_validate_data(csv_file_path):
    """åŠ è½½å¹¶éªŒè¯æ•°æ®"""
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    
    if not os.path.exists(csv_file_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
    
    df = pd.read_csv(csv_file_path)
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    
    return df

def identify_performance_metrics(df):
    """è¯†åˆ«20ä¸ªåŠ¨æ€æ€§èƒ½æŒ‡æ ‡åˆ—"""
    # é™æ€å¸ƒå±€ç‰¹å¾åˆ—ï¼ˆå‰11åˆ—ï¼‰
    static_features = [
        'avg_dist_to_center', 'avg_pairwise_distance', 'std_pairwise_distance',
        'min_pairwise_distance', 'max_pairwise_distance', 'cs_density_std',
        'cluster_count', 'coverage_ratio', 'max_gap_distance',
        'gini_coefficient', 'avg_betweenness_centrality'
    ]
    
    # layout_id åˆ—
    id_column = 'layout_id'
    
    # åŠ¨æ€æ€§èƒ½æŒ‡æ ‡åˆ—ï¼ˆé™¤äº†é™æ€ç‰¹å¾å’Œlayout_idä¹‹å¤–çš„æ‰€æœ‰åˆ—ï¼‰
    all_columns = df.columns.tolist()
    performance_metrics = [col for col in all_columns 
                          if col not in static_features and col != id_column]
    
    print(f"ğŸ” è¯†åˆ«åˆ° {len(performance_metrics)} ä¸ªåŠ¨æ€æ€§èƒ½æŒ‡æ ‡:")
    for i, metric in enumerate(performance_metrics, 1):
        print(f"   {i:2d}. {metric}")
    
    return performance_metrics

def calculate_correlation_matrix(df, metrics, method='pearson'):
    """è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ"""
    print(f"\nğŸ§® è®¡ç®—{method}ç›¸å…³ç³»æ•°çŸ©é˜µ...")
    
    # æå–æ€§èƒ½æŒ‡æ ‡æ•°æ®
    metrics_data = df[metrics].copy()
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    print(f"ğŸ“‹ æ•°æ®è´¨é‡æ£€æŸ¥:")
    print(f"   - æ ·æœ¬æ•°é‡: {len(metrics_data)}")
    print(f"   - æŒ‡æ ‡æ•°é‡: {len(metrics)}")
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_counts = metrics_data.isnull().sum()
    if missing_counts.any():
        print(f"   âš ï¸ å‘ç°ç¼ºå¤±å€¼:")
        for metric, count in missing_counts[missing_counts > 0].items():
            print(f"     - {metric}: {count} ä¸ªç¼ºå¤±å€¼")
        
        # å¡«å……ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨ä¸­ä½æ•°ï¼‰
        metrics_data = metrics_data.fillna(metrics_data.median())
        print(f"   âœ… å·²ä½¿ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼")
    else:
        print(f"   âœ… æ— ç¼ºå¤±å€¼")
    
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    if method == 'pearson':
        corr_matrix = metrics_data.corr(method='pearson')
    elif method == 'spearman':
        corr_matrix = metrics_data.corr(method='spearman')
    else:
        raise ValueError("method å¿…é¡»æ˜¯ 'pearson' æˆ– 'spearman'")
    
    print(f"âœ… {method}ç›¸å…³æ€§çŸ©é˜µè®¡ç®—å®Œæˆ")
    
    return corr_matrix, metrics_data

def calculate_significance_matrix(metrics_data, method='pearson'):
    """è®¡ç®—ç›¸å…³æ€§æ˜¾è‘—æ€§çŸ©é˜µï¼ˆpå€¼ï¼‰"""
    print(f"ğŸ”¬ è®¡ç®—ç›¸å…³æ€§æ˜¾è‘—æ€§ï¼ˆpå€¼ï¼‰...")
    
    n_metrics = len(metrics_data.columns)
    p_matrix = np.ones((n_metrics, n_metrics))
    
    for i in range(n_metrics):
        for j in range(i+1, n_metrics):
            if method == 'pearson':
                _, p_value = pearsonr(metrics_data.iloc[:, i], metrics_data.iloc[:, j])
            else:  # spearman
                _, p_value = spearmanr(metrics_data.iloc[:, i], metrics_data.iloc[:, j])
            
            p_matrix[i, j] = p_value
            p_matrix[j, i] = p_value
    
    p_df = pd.DataFrame(p_matrix, 
                       index=metrics_data.columns, 
                       columns=metrics_data.columns)
    
    print(f"âœ… æ˜¾è‘—æ€§çŸ©é˜µè®¡ç®—å®Œæˆ")
    return p_df

def create_correlation_heatmap(corr_matrix, p_matrix=None, method='pearson', 
                              output_path=None, figsize=(16, 14)):
    """åˆ›å»ºä¸“ä¸šçš„ç›¸å…³æ€§çƒ­åŠ›å›¾"""
    print(f"ğŸ¨ ç”Ÿæˆ{method}ç›¸å…³æ€§çƒ­åŠ›å›¾...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(figsize=figsize)
    
    # åˆ›å»ºmask forä¸Šä¸‰è§’ï¼ˆå¯é€‰ï¼Œæ˜¾ç¤ºå®Œæ•´çŸ©é˜µï¼‰
    # mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    
    # ç”Ÿæˆçƒ­åŠ›å›¾
    sns.heatmap(corr_matrix, 
                # mask=mask,
                annot=True,                    # æ˜¾ç¤ºæ•°å€¼
                fmt='.3f',                     # æ•°å€¼æ ¼å¼
                cmap='RdBu_r',                # é…è‰²æ–¹æ¡ˆï¼šçº¢-ç™½-è“
                center=0,                      # ä¸­å¿ƒå€¼ä¸º0
                square=True,                   # æ­£æ–¹å½¢æ ¼å­
                cbar_kws={"shrink": .8, "label": f"{method.capitalize()} Correlation Coefficient"},
                linewidths=0.5,               # ç½‘æ ¼çº¿å®½åº¦
                ax=ax)
    
    # å¦‚æœæœ‰æ˜¾è‘—æ€§ä¿¡æ¯ï¼Œæ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
    if p_matrix is not None:
        # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®° (* p<0.05, ** p<0.01, *** p<0.001)
        for i in range(len(corr_matrix)):
            for j in range(len(corr_matrix.columns)):
                p_val = p_matrix.iloc[i, j]
                if p_val < 0.001:
                    marker = '***'
                elif p_val < 0.01:
                    marker = '**'
                elif p_val < 0.05:
                    marker = '*'
                else:
                    marker = ''
                
                if marker and i != j:  # ä¸åœ¨å¯¹è§’çº¿ä¸Šæ·»åŠ æ ‡è®°
                    ax.text(j + 0.5, i + 0.8, marker, 
                           ha='center', va='center', 
                           fontsize=8, fontweight='bold', color='black')
    
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    method_name_cn = "çš®å°”é€Š" if method == 'pearson' else "æ–¯çš®å°”æ›¼"
    ax.set_title(f'åŠ¨æ€æ€§èƒ½æŒ‡æ ‡{method_name_cn}ç›¸å…³æ€§çƒ­åŠ›å›¾\n'
                f'Dynamic Performance Metrics {method.capitalize()} Correlation Heatmap\n'
                f'(n=81 layouts)', 
                fontsize=16, fontweight='bold', pad=20)
    
    # æ—‹è½¬xè½´æ ‡ç­¾
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾å½¢
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
        print(f"âœ… çƒ­åŠ›å›¾ä¿å­˜è‡³: {output_path}")
    
    return fig, ax

def create_clustered_heatmap(corr_matrix, method='pearson', output_path=None, figsize=(18, 16)):
    """åˆ›å»ºå¸¦èšç±»çš„ç›¸å…³æ€§çƒ­åŠ›å›¾"""
    print(f"ğŸŒ² ç”Ÿæˆå¸¦å±‚æ¬¡èšç±»çš„{method}ç›¸å…³æ€§çƒ­åŠ›å›¾...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # ä½¿ç”¨ clustermap åˆ›å»ºå¸¦èšç±»çš„çƒ­åŠ›å›¾
    g = sns.clustermap(corr_matrix,
                       annot=True,
                       fmt='.3f',
                       cmap='RdBu_r',
                       center=0,
                       square=True,
                       linewidths=0.5,
                       cbar_kws={"shrink": .8, "label": f"{method.capitalize()} Correlation Coefficient"},
                       figsize=figsize,
                       dendrogram_ratio=0.15,    # æ ‘çŠ¶å›¾æ¯”ä¾‹
                       cbar_pos=(0.02, 0.83, 0.03, 0.15))  # è°ƒæ•´colorbarä½ç½®
    
    # è®¾ç½®æ ‡é¢˜
    method_name_cn = "çš®å°”é€Š" if method == 'pearson' else "æ–¯çš®å°”æ›¼"
    g.fig.suptitle(f'åŠ¨æ€æ€§èƒ½æŒ‡æ ‡{method_name_cn}ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼ˆå¸¦å±‚æ¬¡èšç±»ï¼‰\n'
                   f'Dynamic Performance Metrics {method.capitalize()} Correlation Heatmap with Hierarchical Clustering\n'
                   f'(n=81 layouts)', 
                   fontsize=16, fontweight='bold', y=0.98)
    
    # æ—‹è½¬æ ‡ç­¾
    g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xticklabels(), rotation=45, ha='right')
    g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0)
    
    # ä¿å­˜å›¾å½¢
    if output_path:
        g.savefig(output_path, dpi=300, bbox_inches='tight',
                  facecolor='white', edgecolor='none')
        print(f"âœ… èšç±»çƒ­åŠ›å›¾ä¿å­˜è‡³: {output_path}")
    
    return g

def analyze_correlation_patterns(corr_matrix, threshold=0.7):
    """åˆ†æç›¸å…³æ€§æ¨¡å¼"""
    print(f"\nğŸ” ç›¸å…³æ€§æ¨¡å¼åˆ†æï¼ˆé˜ˆå€¼: |r| â‰¥ {threshold}ï¼‰")
    print("=" * 60)
    
    # æå–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆé¿å…é‡å¤å’Œå¯¹è§’çº¿ï¼‰
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
    corr_values = corr_matrix.where(mask)
    
    # æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„æŒ‡æ ‡å¯¹
    high_corr_pairs = []
    for i in range(len(corr_matrix)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            if abs(corr_val) >= threshold:
                high_corr_pairs.append({
                    'metric1': corr_matrix.index[i],
                    'metric2': corr_matrix.columns[j],
                    'correlation': corr_val,
                    'abs_correlation': abs(corr_val)
                })
    
    # æŒ‰ç»å¯¹ç›¸å…³æ€§å¤§å°æ’åº
    high_corr_pairs.sort(key=lambda x: x['abs_correlation'], reverse=True)
    
    print(f"ğŸ”¥ å‘ç° {len(high_corr_pairs)} å¯¹é«˜ç›¸å…³æ€§æŒ‡æ ‡ï¼ˆ|r| â‰¥ {threshold}ï¼‰ï¼š")
    if high_corr_pairs:
        print(f"{'åºå·':<4} {'æŒ‡æ ‡1':<25} {'æŒ‡æ ‡2':<25} {'ç›¸å…³ç³»æ•°':<10} {'ç±»å‹'}")
        print("-" * 80)
        for i, pair in enumerate(high_corr_pairs, 1):
            corr_type = "æ­£ç›¸å…³" if pair['correlation'] > 0 else "è´Ÿç›¸å…³"
            print(f"{i:<4} {pair['metric1']:<25} {pair['metric2']:<25} "
                  f"{pair['correlation']:<10.3f} {corr_type}")
    else:
        print(f"   æœªå‘ç°ç»å¯¹ç›¸å…³æ€§ â‰¥ {threshold} çš„æŒ‡æ ‡å¯¹")
    
    # ç»Ÿè®¡ç›¸å…³æ€§åˆ†å¸ƒ
    all_corr_values = corr_values.values.flatten()
    all_corr_values = all_corr_values[~np.isnan(all_corr_values)]
    
    print(f"\nğŸ“Š ç›¸å…³æ€§åˆ†å¸ƒç»Ÿè®¡ï¼š")
    print(f"   æ€»æŒ‡æ ‡å¯¹æ•°: {len(all_corr_values)}")
    print(f"   å¹³å‡ç›¸å…³æ€§: {np.mean(np.abs(all_corr_values)):.3f}")
    print(f"   æœ€å¤§æ­£ç›¸å…³: {np.max(all_corr_values):.3f}")
    print(f"   æœ€å¤§è´Ÿç›¸å…³: {np.min(all_corr_values):.3f}")
    print(f"   å¼ºç›¸å…³å¯¹æ•° (|r|â‰¥0.7): {np.sum(np.abs(all_corr_values) >= 0.7)}")
    print(f"   ä¸­ç­‰ç›¸å…³å¯¹æ•° (0.3â‰¤|r|<0.7): {np.sum((np.abs(all_corr_values) >= 0.3) & (np.abs(all_corr_values) < 0.7))}")
    print(f"   å¼±ç›¸å…³å¯¹æ•° (|r|<0.3): {np.sum(np.abs(all_corr_values) < 0.3)}")
    
    return high_corr_pairs

def save_correlation_results(corr_matrix, p_matrix, metrics_data, output_dir, method='pearson'):
    """ä¿å­˜ç›¸å…³æ€§åˆ†æç»“æœ"""
    print(f"\nğŸ’¾ ä¿å­˜{method}ç›¸å…³æ€§åˆ†æç»“æœ...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ç›¸å…³æ€§çŸ©é˜µ
    corr_file = os.path.join(output_dir, f"correlation_matrix_{method}.csv")
    corr_matrix.to_csv(corr_file)
    print(f"âœ… ç›¸å…³æ€§çŸ©é˜µä¿å­˜è‡³: {corr_file}")
    
    # ä¿å­˜æ˜¾è‘—æ€§çŸ©é˜µ
    if p_matrix is not None:
        p_file = os.path.join(output_dir, f"significance_matrix_{method}.csv")
        p_matrix.to_csv(p_file)
        print(f"âœ… æ˜¾è‘—æ€§çŸ©é˜µä¿å­˜è‡³: {p_file}")
    
    # ä¿å­˜æè¿°æ€§ç»Ÿè®¡
    desc_file = os.path.join(output_dir, f"descriptive_statistics_{method}.csv")
    desc_stats = metrics_data.describe()
    desc_stats.to_csv(desc_file)
    print(f"âœ… æè¿°æ€§ç»Ÿè®¡ä¿å­˜è‡³: {desc_file}")
    
    # åˆ›å»ºåˆ†ææŠ¥å‘Š
    report_file = os.path.join(output_dir, f"correlation_analysis_report_{method}.txt")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"åŠ¨æ€æ€§èƒ½æŒ‡æ ‡{method.upper()}ç›¸å…³æ€§åˆ†ææŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"åˆ†ææ–¹æ³•: {method.capitalize()} Correlation\n")
        f.write(f"æ ·æœ¬æ•°é‡: {len(metrics_data)}\n")
        f.write(f"æŒ‡æ ‡æ•°é‡: {len(metrics_data.columns)}\n\n")
        
        f.write("æŒ‡æ ‡åˆ—è¡¨:\n")
        for i, metric in enumerate(metrics_data.columns, 1):
            f.write(f"   {i:2d}. {metric}\n")
        
        f.write("\nç›¸å…³æ€§çŸ©é˜µæ‘˜è¦:\n")
        # æå–ä¸Šä¸‰è§’çŸ©é˜µ
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
        corr_values = corr_matrix.where(mask).values.flatten()
        corr_values = corr_values[~np.isnan(corr_values)]
        
        f.write(f"   æœ€å¤§æ­£ç›¸å…³: {np.max(corr_values):.3f}\n")
        f.write(f"   æœ€å¤§è´Ÿç›¸å…³: {np.min(corr_values):.3f}\n")
        f.write(f"   å¹³å‡ç»å¯¹ç›¸å…³æ€§: {np.mean(np.abs(corr_values)):.3f}\n")
        f.write(f"   å¼ºç›¸å…³å¯¹æ•° (|r|â‰¥0.7): {np.sum(np.abs(corr_values) >= 0.7)}\n")
        f.write(f"   ä¸­ç­‰ç›¸å…³å¯¹æ•° (0.3â‰¤|r|<0.7): {np.sum((np.abs(corr_values) >= 0.3) & (np.abs(corr_values) < 0.7))}\n")
        f.write(f"   å¼±ç›¸å…³å¯¹æ•° (|r|<0.3): {np.sum(np.abs(corr_values) < 0.3)}\n")
    
    print(f"âœ… åˆ†ææŠ¥å‘Šä¿å­˜è‡³: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŠ¨æ€æ€§èƒ½æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ')
    parser.add_argument('--input', type=str, 
                       default='models/input_1-100/merged_dataset.csv',
                       help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, 
                       default='analysis/charts/correlation',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--method', type=str, choices=['pearson', 'spearman', 'both'],
                       default='both', help='ç›¸å…³æ€§è®¡ç®—æ–¹æ³•')
    parser.add_argument('--threshold', type=float, default=0.7,
                       help='é«˜ç›¸å…³æ€§é˜ˆå€¼')
    parser.add_argument('--figsize', type=str, default='16,14',
                       help='å›¾å½¢å°ºå¯¸ (å®½,é«˜)')
    
    args = parser.parse_args()
    
    # è§£æå›¾å½¢å°ºå¯¸
    figsize = tuple(map(int, args.figsize.split(',')))
    
    print("ğŸ” åŠ¨æ€æ€§èƒ½æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    df = load_and_validate_data(args.input)
    
    # è¯†åˆ«æ€§èƒ½æŒ‡æ ‡
    performance_metrics = identify_performance_metrics(df)
    
    if len(performance_metrics) != 20:
        print(f"âš ï¸ è­¦å‘Š: æ£€æµ‹åˆ° {len(performance_metrics)} ä¸ªåŠ¨æ€æ€§èƒ½æŒ‡æ ‡ï¼ŒæœŸæœ›20ä¸ª")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # é€‰æ‹©åˆ†ææ–¹æ³•
    methods = ['pearson', 'spearman'] if args.method == 'both' else [args.method]
    
    for method in methods:
        print(f"\n{'='*20} {method.upper()} ç›¸å…³æ€§åˆ†æ {'='*20}")
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        corr_matrix, metrics_data = calculate_correlation_matrix(df, performance_metrics, method)
        
        # è®¡ç®—æ˜¾è‘—æ€§çŸ©é˜µ
        p_matrix = calculate_significance_matrix(metrics_data, method)
        
        # ç”Ÿæˆæ ‡å‡†çƒ­åŠ›å›¾
        heatmap_path = os.path.join(args.output_dir, f"correlation_heatmap_{method}.png")
        fig1, ax1 = create_correlation_heatmap(corr_matrix, p_matrix, method, 
                                              heatmap_path, figsize)
        plt.close(fig1)
        
        # ç”Ÿæˆèšç±»çƒ­åŠ›å›¾
        clustered_path = os.path.join(args.output_dir, f"correlation_heatmap_clustered_{method}.png")
        g = create_clustered_heatmap(corr_matrix, method, clustered_path, figsize)
        plt.close(g.fig)
        
        # åˆ†æç›¸å…³æ€§æ¨¡å¼
        high_corr_pairs = analyze_correlation_patterns(corr_matrix, args.threshold)
        
        # ä¿å­˜ç»“æœ
        save_correlation_results(corr_matrix, p_matrix, metrics_data, args.output_dir, method)
    
    print(f"\nğŸ‰ ç›¸å…³æ€§åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    print(f"ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬:")
    print(f"   â€¢ ç›¸å…³æ€§çƒ­åŠ›å›¾ (æ ‡å‡†ç‰ˆå’Œèšç±»ç‰ˆ)")
    print(f"   â€¢ ç›¸å…³æ€§çŸ©é˜µ CSV æ–‡ä»¶")
    print(f"   â€¢ æ˜¾è‘—æ€§æ£€éªŒ på€¼çŸ©é˜µ")
    print(f"   â€¢ æè¿°æ€§ç»Ÿè®¡")
    print(f"   â€¢ åˆ†ææŠ¥å‘Š")

if __name__ == '__main__':
    main()
