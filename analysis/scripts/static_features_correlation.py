#!/usr/bin/env python3
"""
11ä¸ªé™æ€å¸ƒå±€ç‰¹å¾ç›¸å…³æ€§åˆ†æ
ç”Ÿæˆä¸“ä¸šçš„ç›¸å…³æ€§çƒ­åŠ›å›¾
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from scipy.stats import pearsonr, spearmanr
import warnings
warnings.filterwarnings('ignore')

def load_static_features():
    """åŠ è½½11ä¸ªé™æ€å¸ƒå±€ç‰¹å¾"""
    # è¯»å–merged_dataset.csv
    data_file = "models/input_1-100/merged_dataset.csv"
    
    if not os.path.exists(data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return None
    
    df = pd.read_csv(data_file)
    print(f"âœ… åŠ è½½æ•°æ®: {df.shape}")
    
    # å‰11åˆ—æ˜¯é™æ€ç‰¹å¾
    static_feature_columns = [
        'avg_dist_to_center',          # å¹³å‡åˆ°ä¸­å¿ƒè·ç¦»
        'avg_pairwise_distance',       # å¹³å‡ä¸¤ä¸¤è·ç¦»  
        'std_pairwise_distance',       # ä¸¤ä¸¤è·ç¦»æ ‡å‡†å·®
        'min_pairwise_distance',       # æœ€å°ä¸¤ä¸¤è·ç¦»
        'max_pairwise_distance',       # æœ€å¤§ä¸¤ä¸¤è·ç¦»
        'cs_density_std',              # å……ç”µæ¡©å¯†åº¦æ ‡å‡†å·®
        'cluster_count',               # èšç±»ç°‡æ•°
        'coverage_ratio',              # è¦†ç›–ç‡
        'max_gap_distance',            # æœ€å¤§ç©ºç™½è·ç¦»
        'gini_coefficient',            # åŸºå°¼ç³»æ•°
        'avg_betweenness_centrality'   # å¹³å‡ä»‹æ•°ä¸­å¿ƒæ€§
    ]
    
    # æå–é™æ€ç‰¹å¾
    static_features = df[static_feature_columns].copy()
    print(f"ğŸ“Š é™æ€ç‰¹å¾: {static_features.shape}")
    print(f"ğŸ“‹ ç‰¹å¾åˆ—è¡¨:")
    for i, col in enumerate(static_feature_columns, 1):
        print(f"   {i:2d}. {col}")
    
    return static_features, static_feature_columns

def create_static_features_heatmap(corr_matrix, method_name, output_path, figsize=(12, 10)):
    """åˆ›å»ºé™æ€ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾"""
    # è®¾ç½®å­—ä½“å‚æ•°
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(figsize=figsize)
    
    # ç”Ÿæˆçƒ­åŠ›å›¾
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # åªæ˜¾ç¤ºä¸‹ä¸‰è§’
    
    sns.heatmap(corr_matrix, 
                mask=mask,                     # é®ç½©ä¸Šä¸‰è§’
                annot=True,                    # æ˜¾ç¤ºæ•°å€¼
                fmt='.3f',                     # æ•°å€¼æ ¼å¼
                cmap='RdBu_r',                # é¢œè‰²æ–¹æ¡ˆ
                center=0,                      # ä¸­å¿ƒå€¼ä¸º0
                square=True,                   # æ­£æ–¹å½¢æ ¼å­
                cbar_kws={"shrink": .8, "label": f"{method_name} Correlation Coefficient"},
                linewidths=0.5,               # ç½‘æ ¼çº¿å®½åº¦
                ax=ax)
    
    # è®¾ç½®æ ‡é¢˜
    ax.set_title(f'Static Layout Features Correlation Matrix\n'
                 f'({method_name} Correlation, n=81 layouts)', 
                fontsize=14, fontweight='bold', pad=20)
    
    # è°ƒæ•´æ ‡ç­¾
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=10)
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=10)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    plt.savefig(output_path, dpi=300, bbox_inches='tight', 
               facecolor='white', edgecolor='none')
    print(f"âœ… {method_name} é™æ€ç‰¹å¾çƒ­åŠ›å›¾ä¿å­˜è‡³: {output_path}")
    
    return fig, ax

def create_clustered_static_heatmap(corr_matrix, method_name, output_path, figsize=(14, 12)):
    """åˆ›å»ºå¸¦èšç±»çš„é™æ€ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾"""
    # è®¾ç½®å­—ä½“å‚æ•°
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºèšç±»çƒ­åŠ›å›¾
    g = sns.clustermap(corr_matrix,
                       annot=True,
                       fmt='.3f',
                       cmap='RdBu_r',
                       center=0,
                       square=True,
                       linewidths=0.5,
                       cbar_kws={"shrink": .8, "label": f"{method_name} Correlation Coefficient"},
                       figsize=figsize,
                       dendrogram_ratio=0.15,
                       cbar_pos=(0.02, 0.83, 0.03, 0.15))
    
    # è®¾ç½®æ ‡é¢˜
    g.fig.suptitle(f'Static Layout Features Correlation with Hierarchical Clustering\n'
                   f'({method_name} Correlation, n=81 layouts)', 
                   fontsize=14, fontweight='bold', y=0.98)
    
    # è°ƒæ•´æ ‡ç­¾
    g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xticklabels(), rotation=45, ha='right', fontsize=10)
    g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0, fontsize=10)
    
    # ä¿å­˜å›¾åƒ
    g.savefig(output_path, dpi=300, bbox_inches='tight',
              facecolor='white', edgecolor='none')
    print(f"âœ… {method_name} èšç±»é™æ€ç‰¹å¾çƒ­åŠ›å›¾ä¿å­˜è‡³: {output_path}")
    
    return g

def calculate_feature_correlations(static_features, method='pearson'):
    """è®¡ç®—ç‰¹å¾é—´çš„ç›¸å…³æ€§"""
    if method == 'pearson':
        corr_matrix = static_features.corr(method='pearson')
    else:
        corr_matrix = static_features.corr(method='spearman')
    
    return corr_matrix

def find_strongest_correlations(corr_matrix, top_n=10):
    """æ‰¾å‡ºæœ€å¼ºçš„ç›¸å…³æ€§å¯¹"""
    correlations = []
    
    # éå†ä¸‹ä¸‰è§’çŸ©é˜µï¼ˆé¿å…é‡å¤å’Œè‡ªç›¸å…³ï¼‰
    for i in range(len(corr_matrix)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            correlations.append({
                'feature1': corr_matrix.index[i],
                'feature2': corr_matrix.columns[j],
                'correlation': corr_val,
                'abs_correlation': abs(corr_val)
            })
    
    # æŒ‰ç»å¯¹ç›¸å…³ç³»æ•°æ’åº
    correlations.sort(key=lambda x: x['abs_correlation'], reverse=True)
    
    return correlations[:top_n]

def create_feature_definitions():
    """åˆ›å»ºç‰¹å¾å®šä¹‰å­—å…¸"""
    feature_definitions = {
        'avg_dist_to_center': 'å¹³å‡åˆ°ä¸­å¿ƒè·ç¦» - å……ç”µæ¡©åˆ°å‡ ä½•ä¸­å¿ƒçš„å¹³å‡è·ç¦»',
        'avg_pairwise_distance': 'å¹³å‡ä¸¤ä¸¤è·ç¦» - æ‰€æœ‰å……ç”µæ¡©å¯¹ä¹‹é—´çš„å¹³å‡è·ç¦»',
        'std_pairwise_distance': 'ä¸¤ä¸¤è·ç¦»æ ‡å‡†å·® - å……ç”µæ¡©å¯¹è·ç¦»çš„å˜å¼‚ç¨‹åº¦',
        'min_pairwise_distance': 'æœ€å°ä¸¤ä¸¤è·ç¦» - æœ€è¿‘å……ç”µæ¡©å¯¹ä¹‹é—´çš„è·ç¦»',
        'max_pairwise_distance': 'æœ€å¤§ä¸¤ä¸¤è·ç¦» - æœ€è¿œå……ç”µæ¡©å¯¹ä¹‹é—´çš„è·ç¦»',
        'cs_density_std': 'å¯†åº¦æ ‡å‡†å·® - ç½‘æ ¼å¯†åº¦åˆ†å¸ƒçš„å˜å¼‚ç¨‹åº¦',
        'cluster_count': 'èšç±»ç°‡æ•° - DBSCANç®—æ³•è¯†åˆ«çš„èšç±»æ•°é‡',
        'coverage_ratio': 'è¦†ç›–ç‡ - 500mèŒƒå›´å†…å¯è¾¾å……ç”µæ¡©çš„è·¯æ®µæ¯”ä¾‹',
        'max_gap_distance': 'æœ€å¤§ç©ºç™½è·ç¦» - è·ç¦»æœ€è¿‘å……ç”µæ¡©æœ€è¿œçš„ç‚¹çš„è·ç¦»',
        'gini_coefficient': 'åŸºå°¼ç³»æ•° - å……ç”µæ¡©æœåŠ¡å¯è¾¾æ€§çš„ä¸å‡åŒ€ç¨‹åº¦',
        'avg_betweenness_centrality': 'å¹³å‡ä»‹æ•°ä¸­å¿ƒæ€§ - å……ç”µæ¡©åœ¨è·¯ç½‘ä¸­çš„å¹³å‡é‡è¦æ€§'
    }
    return feature_definitions

def create_analysis_report(corr_matrix, top_correlations, method_name, output_path):
    """åˆ›å»ºåˆ†ææŠ¥å‘Š"""
    feature_definitions = create_feature_definitions()
    
    report_content = f"""
# é™æ€å¸ƒå±€ç‰¹å¾ç›¸å…³æ€§åˆ†ææŠ¥å‘Š ({method_name})

## åˆ†ææ¦‚è¿°
- **æ ·æœ¬æ•°é‡**: 81ä¸ªå……ç”µç«™å¸ƒå±€æ–¹æ¡ˆ
- **ç‰¹å¾æ•°é‡**: 11ä¸ªé™æ€å¸ƒå±€ç‰¹å¾
- **ç›¸å…³æ€§æ–¹æ³•**: {method_name} ç›¸å…³ç³»æ•°
- **åˆ†ææ—¥æœŸ**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

## ç‰¹å¾å®šä¹‰

"""
    
    for i, (feature, definition) in enumerate(feature_definitions.items(), 1):
        report_content += f"{i:2d}. **{feature}**: {definition}\n"
    
    report_content += f"""

## ç›¸å…³æ€§ç»Ÿè®¡æ‘˜è¦

### ç›¸å…³æ€§å¼ºåº¦åˆ†å¸ƒ
"""
    
    # è®¡ç®—ç›¸å…³æ€§åˆ†å¸ƒ
    all_correlations = []
    for i in range(len(corr_matrix)):
        for j in range(i+1, len(corr_matrix.columns)):
            all_correlations.append(abs(corr_matrix.iloc[i, j]))
    
    very_strong = sum(1 for c in all_correlations if c >= 0.8)
    strong = sum(1 for c in all_correlations if 0.6 <= c < 0.8)
    moderate = sum(1 for c in all_correlations if 0.3 <= c < 0.6)
    weak = sum(1 for c in all_correlations if c < 0.3)
    total = len(all_correlations)
    
    report_content += f"""
- **æå¼ºç›¸å…³ (|r| â‰¥ 0.8)**: {very_strong} å¯¹ ({very_strong/total*100:.1f}%)
- **å¼ºç›¸å…³ (0.6 â‰¤ |r| < 0.8)**: {strong} å¯¹ ({strong/total*100:.1f}%)
- **ä¸­ç­‰ç›¸å…³ (0.3 â‰¤ |r| < 0.6)**: {moderate} å¯¹ ({moderate/total*100:.1f}%)
- **å¼±ç›¸å…³ (|r| < 0.3)**: {weak} å¯¹ ({weak/total*100:.1f}%)

### æœ€å¼ºç›¸å…³æ€§å¯¹ (Top 10)

"""
    
    for i, corr in enumerate(top_correlations, 1):
        corr_type = "æ­£ç›¸å…³" if corr['correlation'] > 0 else "è´Ÿç›¸å…³"
        report_content += f"{i:2d}. **{corr['feature1']}** â†” **{corr['feature2']}**\n"
        report_content += f"    - ç›¸å…³ç³»æ•°: {corr['correlation']:.3f} ({corr_type})\n"
        report_content += f"    - ç»å¯¹å€¼: {corr['abs_correlation']:.3f}\n\n"
    
    report_content += """
## å…³é”®å‘ç°

### 1. è·ç¦»ç±»ç‰¹å¾é«˜åº¦ç›¸å…³
- å¹³å‡ä¸¤ä¸¤è·ç¦»ã€æ ‡å‡†å·®ã€æœ€å¤§è·ç¦»ç­‰ç©ºé—´åˆ†å¸ƒç‰¹å¾è¡¨ç°å‡ºå¼ºç›¸å…³æ€§
- åæ˜ äº†å……ç”µæ¡©ç©ºé—´åˆ†å¸ƒçš„ä¸€è‡´æ€§æ¨¡å¼

### 2. è¦†ç›–æ€§æŒ‡æ ‡çš„å…³è”
- è¦†ç›–ç‡ã€æœ€å¤§ç©ºç™½è·ç¦»ã€åŸºå°¼ç³»æ•°ç­‰æœåŠ¡è¦†ç›–æŒ‡æ ‡ç›¸äº’å…³è”
- ä½“ç°äº†ä¸åŒè¦†ç›–æ€§åº¦é‡çš„ä¸€è‡´æ€§

### 3. ç½‘ç»œæ‹“æ‰‘ç‰¹å¾çš„ç‹¬ç‰¹æ€§
- ä»‹æ•°ä¸­å¿ƒæ€§ä½œä¸ºç½‘ç»œæ‹“æ‰‘ç‰¹å¾ï¼Œä¸å…¶ä»–å‡ ä½•ç‰¹å¾ç›¸å…³æ€§è¾ƒä½
- æä¾›äº†ç‹¬ç‰¹çš„å¸ƒå±€è¯„ä¼°ç»´åº¦

### 4. èšç±»ç‰¹å¾çš„ä¸­ä»‹ä½œç”¨
- èšç±»ç°‡æ•°ä¸å¤šä¸ªç©ºé—´åˆ†å¸ƒç‰¹å¾å­˜åœ¨ä¸­ç­‰ç›¸å…³æ€§
- æ˜¯è¿æ¥å‡ ä½•ç‰¹å¾å’Œè¦†ç›–æ€§ç‰¹å¾çš„æ¡¥æ¢

## åº”ç”¨å»ºè®®

1. **ç‰¹å¾é€‰æ‹©**: åœ¨æœºå™¨å­¦ä¹ å»ºæ¨¡ä¸­ï¼Œå¯è€ƒè™‘å»é™¤é«˜åº¦ç›¸å…³çš„å†—ä½™ç‰¹å¾
2. **å¸ƒå±€è¯„ä¼°**: ç»“åˆä¸åŒç±»å‹çš„ç‰¹å¾ï¼ˆè·ç¦»ã€è¦†ç›–ã€ç½‘ç»œï¼‰è¿›è¡Œç»¼åˆè¯„ä¼°
3. **è®¾è®¡ä¼˜åŒ–**: å…³æ³¨ç›¸å…³æ€§è¾ƒä½çš„ç‰¹å¾ç»„åˆï¼Œå®ç°å¤šç»´åº¦ä¼˜åŒ–
"""
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"âœ… åˆ†ææŠ¥å‘Šä¿å­˜è‡³: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š 11ä¸ªé™æ€å¸ƒå±€ç‰¹å¾ç›¸å…³æ€§åˆ†æ")
    print("=" * 50)
    
    # åŠ è½½é™æ€ç‰¹å¾æ•°æ®
    static_features, feature_columns = load_static_features()
    if static_features is None:
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "analysis/charts/correlation/static_features"
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¡ç®—Pearsonå’ŒSpearmanç›¸å…³æ€§
    methods = ['pearson', 'spearman']
    
    for method in methods:
        print(f"\nğŸ” è®¡ç®— {method.capitalize()} ç›¸å…³æ€§...")
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        corr_matrix = calculate_feature_correlations(static_features, method)
        
        # ä¿å­˜ç›¸å…³æ€§çŸ©é˜µ
        corr_csv_path = os.path.join(output_dir, f"static_features_correlation_{method}.csv")
        corr_matrix.to_csv(corr_csv_path)
        print(f"ğŸ’¾ ç›¸å…³æ€§çŸ©é˜µä¿å­˜è‡³: {corr_csv_path}")
        
        # ç”Ÿæˆæ ‡å‡†çƒ­åŠ›å›¾
        heatmap_path = os.path.join(output_dir, f"static_features_heatmap_{method}.png")
        fig, ax = create_static_features_heatmap(corr_matrix, method.capitalize(), heatmap_path)
        plt.close(fig)
        
        # ç”Ÿæˆèšç±»çƒ­åŠ›å›¾
        clustered_path = os.path.join(output_dir, f"static_features_heatmap_clustered_{method}.png")
        g = create_clustered_static_heatmap(corr_matrix, method.capitalize(), clustered_path)
        plt.close(g.fig)
        
        # æ‰¾å‡ºæœ€å¼ºç›¸å…³æ€§
        top_correlations = find_strongest_correlations(corr_matrix, top_n=10)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report_path = os.path.join(output_dir, f"static_features_analysis_report_{method}.md")
        create_analysis_report(corr_matrix, top_correlations, method.capitalize(), report_path)
        
        # æ˜¾ç¤ºå‰5ä¸ªæœ€å¼ºç›¸å…³æ€§
        print(f"\nğŸ”¥ {method.capitalize()} æœ€å¼ºç›¸å…³æ€§ (Top 5):")
        for i, corr in enumerate(top_correlations[:5], 1):
            corr_type = "æ­£ç›¸å…³" if corr['correlation'] > 0 else "è´Ÿç›¸å…³"
            print(f"   {i}. {corr['feature1']} â†” {corr['feature2']}")
            print(f"      ç›¸å…³ç³»æ•°: {corr['correlation']:.3f} ({corr_type})")
    
    print(f"\nğŸ‰ é™æ€ç‰¹å¾ç›¸å…³æ€§åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š ç”Ÿæˆæ–‡ä»¶:")
    print(f"   â€¢ static_features_heatmap_pearson.png - Pearsonçƒ­åŠ›å›¾")
    print(f"   â€¢ static_features_heatmap_spearman.png - Spearmançƒ­åŠ›å›¾")
    print(f"   â€¢ static_features_heatmap_clustered_pearson.png - Pearsonèšç±»çƒ­åŠ›å›¾")
    print(f"   â€¢ static_features_heatmap_clustered_spearman.png - Spearmanèšç±»çƒ­åŠ›å›¾")
    print(f"   â€¢ static_features_correlation_pearson.csv - Pearsonç›¸å…³æ€§çŸ©é˜µ")
    print(f"   â€¢ static_features_correlation_spearman.csv - Spearmanç›¸å…³æ€§çŸ©é˜µ")
    print(f"   â€¢ static_features_analysis_report_pearson.md - Pearsonåˆ†ææŠ¥å‘Š")
    print(f"   â€¢ static_features_analysis_report_spearman.md - Spearmanåˆ†ææŠ¥å‘Š")

if __name__ == '__main__':
    main()
